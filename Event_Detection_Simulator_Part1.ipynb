{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mojtabaSefidi/AI-Challenge-PoliceVsThieves/blob/main/Event_Detection_Simulator_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjmLd8KFafKP"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlAIr3hKkfVr",
        "outputId": "c3f8281f-5c4c-499e-a9e8-3d7e8d7575f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haJgbMelaqGH"
      },
      "source": [
        "### Import & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmWnO-4U39yy",
        "outputId": "84b2cf60-86c5-45ea-980f-2d8875c96f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR0zNjgWoHYT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "import pickle\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JLqBfsYnCfs",
        "outputId": "43da77d7-a72e-4488-ae74-861c7ec65e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Jh1BN0oxWS"
      },
      "outputs": [],
      "source": [
        "# !pip install keybert[all]\n",
        "# from keybert import KeyBERT\n",
        "# kw_model = KeyBERT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT696ztGlEOk",
        "outputId": "89651be0-ec24-458e-83c5-db697c24d70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1LDlffCuIatgoCKlxGInTh8W3b1fYgJ_y \n",
            "\n",
            "unzip:  cannot find or open resources.zip, resources.zip.zip or resources.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "from hazm import *\n",
        "from hazm import stopwords_list\n",
        "!gdown 1LDlffCuIatgoCKlxGInTh8W3b1fYgJ_y\n",
        "!unzip resources.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4D1Nf-228mX",
        "outputId": "2414f464-e0a2-4980-af24-a0b609d392c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/HooshvareLab_bert-base-parsbert-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/HooshvareLab_bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "Pars_Bert_Model = SentenceTransformer('HooshvareLab/bert-base-parsbert-uncased',device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBiC_-_2a7jN"
      },
      "source": [
        "## Define Positive , Negative & Spam Key-Phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NioYnygWlzf"
      },
      "source": [
        "### Stop Words & Spam Key Phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_NzoPGnXmQd"
      },
      "outputs": [],
      "source": [
        "stop_words = set(['زیرا','از','به','در','چه','پس','ولی','چون','یا','نیز','را','و', 'هم' , 'اگر' ,'تا' ,'اما' , 'که', 'با', 'اگرچه', 'آن', 'این', 'اکنون','همین','همان','چنانچه','آ‌نگاه','چنانچه','چنانچه','بر','لایک', 'برای','است','آیا'])\n",
        "\n",
        "spam_keyphrase = ['پادکست','نظرسنجی','تلگرام','ترکیب']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihTbRnJtXcSm"
      },
      "source": [
        "### Sport Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hEGiRVFXpul"
      },
      "outputs": [],
      "source": [
        "list_of_sport_positive_keyphrase = [\n",
        "                           ['مدال','طلا'],['مدال','برنز'],['مدال','نقره'],\n",
        "                           ['مسابقه','فینال'],['تیم','قهرمانی','مسابقات'],\n",
        "                           ['ورزشکار','محرومیت'],['تیم','قهرمانی','جام'],\n",
        "                           ['دوپینگ','تست','مثبت'],['تیم','قهرمان','مسابقات'],\n",
        "                           ['بازیکن','توافق','تیم'],['ساله','قرارداد','توافق'],['امضای','قرارداد'],\n",
        "                           ['فسخ','قرارداد','کرد'],['فسخ','قرارداد','شد'],['قرارداد','تمدید','کرد'],\n",
        "                           ['انتقالات','نقل','بمب','توافق'],['انتقالات','نقل','بمب','پیوست'],\n",
        "                           ['قرضی','قرارداد'],['جایزه','بهترین'],['برگزاری','مسابقات'],\n",
        "                           ['لغو','مسابقات'],['تعویق','مسابقات'],['برگزار','مسابقات'],\n",
        "                           ['رکورد','گینس'],['رکورد','المپیک','شکسته'],['رکورد','جهان','شکسته'],\n",
        "                           ['رکورد','آسیا','شکسته'],['رکورد','المپیک','شکست'],\n",
        "                           ['رکورد','جهان','شکست'],['رکورد','آسیا','شکست'],\n",
        "                           ['رکورد','شکسته','شد'],['رکورد','به','حمله'],['در','یک','قدمی'],\n",
        "                           ['پناهجو','ورزشکار'],['پناهنده'],['هاله','از','ابهام'],['همکاری','قطع'],\n",
        "                           ['اخراج','سرمربی'],['استعفا','سرمربی'],['شد','سرمربی','موقت'],\n",
        "                           ['بار','اولین','برای'],['ستاره','درخشش'],['باورنکردنی'],\n",
        "                           ['بانوان','استادیوم'],['مصدومیت','شدید'],['مسابقات','صعود'],['کرد','سقوط'],\n",
        "                           ['رقابت','صعود'],['تست','کرونای','مثبت'],['کسب','سهمیه'],\n",
        "                           ['تست','کرونا','مثبت'],['کمیته','انضباطی','رای'],['کمیته','انضباطی','اخضار'],\n",
        "                           ['محرومیت','انضباطی'],['شعار','هواداران'],['اعتراض','هواداران'],\n",
        "                           ['اعتراض','شدید'],['محرومیت','ساله'],['پرونده','جریمه'],['شد','قطعی'],\n",
        "                           ['خداحافظی','هواداران'],['خداحافظی','تیم'],['وداع','تیم'],['در','قدمی','یک'],\n",
        "                           ['فوت','کرد'],['فوت','شد'],['در','جدایی','آستانه'],['گل','آقای','شد'],\n",
        "                           ['پزشکی','تست'],['راهی','بیمارستان'],['شد','بیهوش'],['تیغ','جراحی'],['تیغ','جراحان'],\n",
        "                           ['شد','رونمایی'],['کرد','رونمایی'],['میلیون','طلب'],['منصوب','شد'],\n",
        "                           ['طلا','توپ','برنده'],['طلا','کفش','برنده'],['هواداران','هجوم'],\n",
        "                           ['میادین','بازگشت','به'],['میادین','دوری','از'],['تاریخی','رکورد'],\n",
        "                           ['لو','رفت'],['مشخص','شد','تاریخ'],['مشخص','شد','قیمت'],['شد','تصویب'],\n",
        "                           ['مشخص','شد','جانشین'],['مشخص','شد','سرمربی'],['مشخص','شد','لیست'],['مشخص','شد','زمان'],\n",
        "                           ['مشخص','شد','قهرمان'],['مشخص','شدند','نفرات','برتر'],['مشخص','شد','نتایج'],['ناباورانه'],\n",
        "                           ['شدید','اعتراض'],['شدید','انتقاد'],['فوری'],['کشور','ثالث','بازی'],['شدید','درگیری'],\n",
        "                           ['قلبی','ایست'],\n",
        "                                 \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfOvmZWW49y"
      },
      "source": [
        "### Technology Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlbE_tfZX0v8"
      },
      "outputs": [],
      "source": [
        "list_of_technology_positive_keyphrase = [\n",
        "                           ['جدید','منتشر','نسخه'],['رونمایی','نسخه'],\n",
        "                           ['سایبری','حمله'],['هک','شد'],['حمله','هکر'],\n",
        "                           ['حمله','هکرها'],['اطلاعات','افشای'],['لو','رفت'],\n",
        "                           ['لو','رفتن'],['برگزار','رویداد'],['بار','اولین','برای'],\n",
        "                           ['ماهواره'],['نوآوری','جدید'],['رونمایی','شد'],\n",
        "                           ['رونمایی','کرد'],['تاریخ','عرضه'],['معرفی','شد'],\n",
        "                           ['غول','فناوری'],['گروه','هکری'],['آپدیت','بزرگ'],\n",
        "                           ['حفره','امنیتی'],['آپدیت','جدید'],['اختلال'],['باگ','امنیتی'],\n",
        "                           ['نفوذ','سایبری'],['پشتیبانی','میکند'],['عرضه','شد'],['عرضه','میشود'],\n",
        "                           ['جدید','سری'],['ارز','نیمایی'],['خرید','شرکت'],['کرد','شکایت'],\n",
        "                           ['سرویس','فعال'],['سرویس','جدید'],['قابلیت','جدید'],['با','حضور'],\n",
        "                           ['افتتاح','مراسم'],['میلیون','دلار'],['راه','اندازی','کرد'],['نسخه','آزمایشی'],\n",
        "                           ['معرفی','بهترین'],['فناوری','جدید'],['منصوب','شد'],['مدل','جدید'],\n",
        "                           ['عملکرد','بهبود'],['آپدیت','بهبود'],['نسل','جدید','معرفی'],['افزایش','امنیت'],\n",
        "                           ['جدیدترین','رونمایی'],['انتشار','نسخه'],['شرکت','سهام','افزایش'],['شرکت','سهام','رشد'],\n",
        "                           ['گوشی','پرچمدار','معرفی'],['گوشی','پرچمدار','عرضه'],['گوشی','پرچمدار','منتشر'],\n",
        "                           ['گوشی','پرچمدار','عرضه'],['فوری'],['افزایش','قیمت'],['کاهش','قیمت'],['چشمگیر'],\n",
        "                           ['تریلیون','ارزش'],['گمرکی','نرخ'],['مجهز','شد'],['جدید','تکنولوژی'],['بروزرسانی','جدید'],\n",
        "                           ['روز','جدید','رسانی'],['جدید','امکانات'],['گجت','هوشمند'],['معرفی','محصول','جدید'],\n",
        "                           ['تاریخی','رکورد'],['استقبال','چشمگیر'],['استقبال','چشمگیر'],['مشخص','شد','تاریخ'],\n",
        "                           ['مشخص','شد','قیمت'],['مشخص','شد','زمان'],['تخصصی','نمایشگاه'],['خارج','دسترس','شد'],\n",
        "                           ['تولید','توقف'],['کرد','تکذیب'],['رقابت','معرفی'],['شد','تصویب'],\n",
        "                           ['رکورد','گینس'],['رکورد','شکست'],['رکورد','فروش'],['گزارش','مالی','منتشر'],\n",
        "                           ['جدید','پلتفرم'],['کرد','متحول'],['جدید','پلتفرم'],['شد','متحول'],['بزرگ','آپدیت'],\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RQwJqwjW5Cj"
      },
      "source": [
        "### Art Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDABXp1TX2Uc"
      },
      "outputs": [],
      "source": [
        "list_of_art_positive_keyphrase = [\n",
        "['سکانس','منتشر'],['سکانس','انتشار'],['فیلم','انتشار'],['سریال','انتشار'],['فیلم','منتشر'],['سریال','منتشر'],\n",
        "['پیکر','تشییع'],['تشییع','خاکسپاری'],['بازیگران','فیلم','منتشر'],['انیمیشن','فروش'],['فیلم','فروش'],['سریال','فروش'],\n",
        "['فیلم','اکران'],['کاهش','کانون','عضو'],['فیلم','درآمد'],['دار','فانی','وداع'],['فوت','کرد'],['فوت','شد'],['سینما','گرفتار'],\n",
        "['فیلم','ممنوع'],['سریال','ممنوع'],['شرکت','جشنواره'],['کنسرت','لغو'],['برگزار','برنامه'],['مجوز','کنسرت'],['مجوز','فیلم'],\n",
        "['مجوز','اکران'],['سریال','جدید'],['فیلم','جدید'],['اکران','تعویق'],['نمایش','عمومی','تعویق'],['برنامه','تولید'],['مسابقه','تولید'],\n",
        "['مسابقه','تقلب'],['تولید','منتشر'],['ساخت','آثار'],['سریال','پخش'],['فیلم','پخش'],['کنسرت','برگزار'],['برگزاری','کنسرت'],\n",
        "['بازگشت','تلویزیون'],['ساخت','محصولات','طنز'],['ساخت','محصولات','کمدی'],['انتقاد','سینما'],['قطعه','موسیقی','منتشر'],\n",
        "['قطعه','موسیقی','پخش'],['برنده','جایزه','بهترین'],['سینما','اکران'],['ابطال','مجوز'],['تعلیق','مجوز'],['اعتراض','واکنش'],\n",
        "['برگزاری','جشنواره'],['انتخاب','بهترین'],['سینما','خداحافظی'],['تلویزیون','خداحافظی'],['انتشار','کتاب'],['کتاب','منتشر'],\n",
        "['انتشار','مجله','هنری'],['مجله','هنری','منتشر'],['بازدید','نمایشگاه'],['مراسم','برگزار'],['واکنش','هنرمندان'],\n",
        "['قیمت','بلیت','سینما'],['کمبود','کتابفروشی'],['استعدادیابی','موسیقی'],['استعدادیابی','خوانندگی'],['استعدادیابی','بازیگری'],\n",
        "['فروش','آثار','هنری'],['حذف','صدا','سیما'],['فیلم','ایفا','نقش'],['سریال','ایفا','نقش'],['ساخت','سریال'],\n",
        "['ساخت','فیلم'],['سانسور','سکانس'],['سانسور','فیلم'],['سانسور','سریال'],['ساخت','موسیقی'],['انتشار','موسیقی'],\n",
        "['موسیقی','پخش'],['فعالیت','آغاز'],['وارد','سینماشد'],['قیمت','بلیت','کنسرت'],['انتشار','اشعار'],['انتشار','شعر'],\n",
        "['کتاب','حراج'],['فروش','کتاب'],['کتاب','صادر'],['اشعار','انتقاد'],['ترجمه','کتاب'],['تحلیل','فیلم'],\n",
        "['تحلیل','سریال'],['تحلیل','کتاب'],['چاپ','اثر'],['چاپ','آثار'],['انتخاب','اعضا','شورا'],['تور','کنسرت'],\n",
        "['میلیون','دلار'],['میلیارد','دلار'],['صدرنشین','فروش'],['رکورد','فروش'],['سرسام','آوری'],['سرسام','آور'],\n",
        "['معرفی','شد'],['معرفی','کرد'],['تحسین','برانگیزی'],['تحسین','برانگیز'],['بهترین','سال'],['اهدای','جوایز'],\n",
        "['اهدا','کرد'],['پشتیبانی','میکند'],['پشتیبانی','می‌کند'],['پشتیبانی','میکنند'],['پشتیبانی','می‌کنند'],['نسخه','فروش'],\n",
        "['رکورد','فروش'],['میلیون','کاربر'],['میلیون','فروش'],['میلیارد','فروش'],['به‌روزرسانی'],['پشتیبانی','کرد'],\n",
        "['منتشر','شد'],['منتشر','کرد'],['منتشر','میشود'],['منتشر','می‌شود'],['پرفروش‌ترین‌'],['کاهش','تقاضا'],['افزایش','تقاضا'],\n",
        "['رسمی'],['لو','رفت'],['خرید','شرکت'],['تاریخ','انتشار'],['رونمایی'],['اعتراض'],['تعطیل','شد'],['هزینه','میلیون'],\n",
        "['هزینه','میلیارد'],['انتشار','اولین','تصاویر'],['برگزاری','حراجی'],['شایعه'],['برگزاری','رویداد'],['برگزار','رویداد'],['کشف','حجاب'],\n",
        "['مهاجرت','کرد'],['پناهنده','شد'],['تاخیر','خورد'],['حالت','تعلیق'],['حذف','شد'],['حذف','شدند'],['حذف','کرد'],['انتشار','بیانیه‌'],\n",
        "['انتشار','بیانیه‌ای'],['خرید','کمپانی'],['مالکیت','دست','داد'','],['فاش','شد'],['فاش','کرد'],['فاش','شدند'],['عذرخواهی','کرد'],\n",
        "['عذرخواهی','کردند'],['شایعات'],['افشای','اطلاعات'],['موفقیت','خیره‌کننده'],['دستمزد','نجومی'],['ممنوع','الکار'],['موفقیت','غیرمنتظره‌'],\n",
        "['تحریم','کرد'],['تحریم','شد'],['خرید','استودیو'],['فروش','استودیو'],['تصاحب','کرد'],['تصمیم','جنجالی'],['درگیری','شدید'],\n",
        "['استقبال','شدید'],['استقبال','چشمگیر'],['استقبال','چشم‌گیر'],['حاضر','مصاحبه','نشد'],['انتقاد','شدید'],['ممنوع','شد'],['تحریم','کرد'],\n",
        "['تحریم','شد'],['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],['اخراج','شد'],\n",
        "['اخراج','کرد'],['تصویب','شد'],['تصویب','کرد'],['تخلف','محرز'],['اختلاس'],['تغییرات','مدیریتی'],['اسرع','وقت'],['برخورد','متخلفان'],\n",
        "['اقدامات','قانونی'],['برخورد','جدی'],['برگزاری','نمایشگاه'],['برگزار','نمایشگاه'],['برگزاری','رویداد'],['برگزار','رویداد'],['افتتاح'],\n",
        "['تفاهم‌نامه','امضا'],['توافق','کرد'],['اولین','رسمی'],['آغاز','عرضه'],['پایان','عرضه'],['امضای','توافق','نامه'],['امضای','توافقنامه'],\n",
        "['امضای','توافق‌نامه'],['استقبال','بینظیر'],['استقبال','بی‌نظیر'],['استقبال','بی','نظیر'],['پلمپ','شد'],['پلمپ','کرد'],['آتش','سوزی'],['آتش‌سوزی'],\n",
        "['حریق','دچار']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z8pPJwZW5IC"
      },
      "source": [
        "### Health Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKoHGQc8X4mc"
      },
      "outputs": [],
      "source": [
        "list_of_health_positive_keyphrase = [\n",
        "    ['واردات','دارو'],['مستقر','شدند'],['مستقر','شده'],['وزیر','بهداشت'],['اعزام','پزشکان'],['اعزام','بازرسان'],\n",
        "    ['اعزام','بیماران'],['معاون','وزارت','بهداشت'],['معاون','وزیر','بهداشت'],['کنگره','برگزار','میشود'],\n",
        "    ['کنگره','برگزار','شده'],['کنگره','برگزار','شد'],['کنگره','برگزار','شود'],['جشنواره','برگزار','میشود'],\n",
        "    ['جشنواره','برگزار','شده'],['جشنواره','برگزار','شد'],['جشنواره','برگزار','شود'],['رئیس','سازمان','غذا','دارو'],\n",
        "    ['توزیع','واکسن'],['منصوب','کرد'],['منصوب','شد'],['افتتاح'],['بازدید','از','بیمارستان'],['بازدید','از','پروژه'],\n",
        "    ['پروژه','نیمه','کاره'],['قاچاق','دارو'],['ارسال','محموله'],['اهدای','دارو'],['اهدای','محموله'],['تسلیت','گفت'],\n",
        "    ['کمک','های','بشر','دوستانه'],['کشته','شدن'],['کشته','شدند'],['آماده','باش'],['تنش','آبی'],['اهدای','آب','آشامیدنی'],\n",
        "    ['کمیته','انضباطی'],['لغو','پروانه'],['نامه','اعتراضی'],['وقوع','حادثه'],['اعزام','تیم','نجات'],['اعزام','گروه','نجات'],\n",
        "    ['انتقال','به','منطقه','امن'],['منتقل','منطقه','امن'],['وقوع','حادثه'],['مدیریت','بحران'],['تشکیل','جلسه'],\n",
        "    ['پیدا','شدن','پیکر'],['مفقود','شدن'],['مفقود','شدند'],['واردات','واکسن'],['ممنوع','شدن'],['شناسایی','بیمار'],\n",
        "    ['جان','باخت'],['جان','باختند'],['امضای','تفاهم','نامه'],['تولید','دارو'],['برای','اولین','بار'],['تامین','دستگاه'],\n",
        "    ['تامین','تجهیزات'],['مناطق','محروم'],['روند','کاهشی'],['روند','افزایشی'],['رئیس‌جمهور'],['رئیس‌','جمهور'],['رییس‌جمهور'],\n",
        "    ['رییس‌','جمهور'],['مهار','شد'],['مهار','شدند'],['همه','گیری','بیماری'],['همه','گیری','ویروس'],['اختصاص','بودجه'],\n",
        "    ['پرداخت','هزینه'],['تصویب','شد'],['تصویب','رسید'],['تصویب','شدند'],['کاهش','فوتی'],['افزایش','فوتی'],['کشور','برتر','دنیا'],\n",
        "    ['طبق','اعلام','سازمان'],['کاهش','هزینه'],['افزایش','هزینه'],['اجرا','شدن','سراسری'],['اجرا','کردن','سراسری'],\n",
        "    ['اجرا','کنیم','سراسری'],['اجرای','سراسری'],['رکورد','بینظیر'],['رکورد','بی','نظیر'],['رکورد','بی‌نظیر'],['کاهش','مرگ','میر'],\n",
        "    ['افزایش','مرگ','میر'],['نخستین','مورد','ابتلا'],['اجرای','طرح'],['تحریم','دارو'],['صادرات','دارو'],['صادرات','واکسن'],\n",
        "    ['دارو','وارد','شد'],['دارو','وارد','کرد'],['دارو','صادر','کرد'],['دارو','صادر','شد'],['بهره','برداری'],['تقدیر','شد'],\n",
        "    ['تقدیر','کرد'],['امضای','تفاهم‌نامه'],['ورود','محموله'],['خارج','محموله'],['خروج','محموله'],['قابل','تحسین'],['حمایت','طرح'],\n",
        "    ['تشکیل','کمیته'],['پرداخت','بدهی'],['خبر','داد'],['بازدید','پروژه'],['قوی','ترین','قدرت','منطقه'],['قوی‌ترین','قدرت','منطقه'],\n",
        "    ['تصویب','آئین','نامه'],['دچار','مشکل'],['ضرورت','تقویت'],['ضرورت','تشکیل'],['بهترین','حوزه'],['تسلیت','گفت'],['فوت','کرد'],\n",
        "    ['فوت','شد'],['حادثه','دلخراش'],['حادثه','دل','خراش'],['حادثه','دل‌خراش'],['وقوع','حادثه'],['محدودیت','جدید'],['فرصت','جبران'],\n",
        "    ['خطوط','قرمز','وزارت'],['خط','قرمز','وزارت'],['ضروری','است'],['راه','اندازی','سامانه'],['راه','اندازی','سیستم'],['موظف','اجرای'],\n",
        "    ['موظف','اعطای'],['اختصاص','بودجه'],['تخصیص','بودجه'],['افزایش','تحت','پوشش'],['رسیدن','خودکفایی'],['خودکفایی','می‌رسیم'],\n",
        "    ['خودکفایی','می‌','رسیم'],['تاسیس','میشوند'],['تاسیس','شدند'],['تاسیس','شد'],['لزوم','اجرای'],['افزایش','موارد','ابتلا'],['اقدامات','تحسین','برانگیز']\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0euC6vyW5Nq"
      },
      "source": [
        "### Economy Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuL7UFJiX7z3"
      },
      "outputs": [],
      "source": [
        "list_of_economy_positive_keyphrase = [\n",
        "                           ['شد','گران'],['شد','ارزان'],\n",
        "                           ['قیمت','کاهش'],['قیمت','افزایش'],\n",
        "                           ['نرخ','کاهش','تورم'],['نرخ','افزایش','تورم'],\n",
        "                           ['نرخ','کاهش'],['نرخ','افزایش'],\n",
        "                           ['تقاضا','افزایش'],['تقاضا','کاهش'],\n",
        "                           ['عرضه','افزایش'],['عرضه','کاهش'],\n",
        "                           ['حذف','یارانه'],['اقتصاد','وزیر'],['رشد', 'اقتصادی'],\n",
        "                           ['مهار', 'تورم'],['کنترل', 'تورم'],['رشد', 'صادرات'],\n",
        "                           ['واردات', 'رشد'],['واردات', 'کاهش'],['کاهش', 'صادرات'],\n",
        "                           ['نرخ', 'تورم'],['تولید', 'سقوط'],['تولید', 'کاهش'],['تولید', 'افزایش'],\n",
        "                           ['کاهش', 'قیمت'],['افزایش', 'قیمت'],['رشد', 'قیمت'],['شاخص', 'کل', 'رشد'],\n",
        "                           ['شاخص', 'کل', 'افزایش'],['شاخص', 'کل', 'افت'],['شاخص', 'کل', 'کاهش'],\n",
        "                           ['شاخص', 'کل', 'مثبت'],['شاخص', 'کل', 'منفی'],['شاخص', 'هم' ,'وزن', 'رشد'],\n",
        "                           ['شاخص', 'هم' ,'وزن', 'افزایش'],['شاخص', 'هم', 'وزن', 'افت'],['شاخص', 'هم' ,'وزن', 'کاهش'],\n",
        "                           ['شاخص', 'هم' ,'وزن', 'مثبت'],['شاخص', 'هم' ,'وزن', 'منفی'],\n",
        "                           ['ارزش', 'معاملات', 'بورس'],['ارزش', 'معاملات', 'خرد'],\n",
        "                           ['سرمایه', 'گذاری', 'اقتصاد'],['رشد', 'تجارت'],['کاهش', 'تجارت'],['رشد', 'تقاضا'],\n",
        "                           ['رشد', 'عرضه'],['رشد', 'کسب' , 'کار'],['افزایش', 'تقاضا'],['افزایش', 'عرضه'],\n",
        "                           ['افزایش', 'کسب' ,'کار'],['کاهش', 'تقاضا'],['کاهش', 'عرضه'],['کاهش', 'کسب' , 'کار'],\n",
        "                           ['بهبود', 'تورم'],['کاهش', 'تورم'],['افزایش', 'تورم'],['رکود', 'تورم'],['افت', 'قیمت'],\n",
        "                           ['اصلاح', 'قیمت'],['معاملات', 'رشد'],['معاملات', 'افزایش'],['معاملات', 'کاهش'],\n",
        "                           ['بورس', 'انرژی'],['بازار', 'سرمایه'],['تورم', 'درصد'],['هزینه', 'کاهش'],\n",
        "                           ['هزینه', 'افزایش'],['سود', 'بانک'],['بورس', 'رشد'],['بورس', 'کاهش'],\n",
        "                           ['افت', 'دلار'],['افزایش', 'دلار'],['کاهش', 'دلار'],['رشد', 'دلار'],['یارانه', 'حذف'],\n",
        "                           ['رشد', 'نقدینگی'],['کاهش', 'نقدینگی'],['کاهش', 'نرخ'],['افزایش', 'نرخ'],\n",
        "                           ['رشد', 'پایه', 'پولی', 'کاهش'],['رشد', 'پایه', 'پولی', 'افزایش'],['ارزان', 'شد'],\n",
        "                           ['گران', 'شد'],['نرخ', 'افزایش', 'تورم'],['نرخ', 'کاهش', 'تورم'],['اقتصاد', 'وزیر'],\n",
        "                                  \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVHC2-q8XLU8"
      },
      "source": [
        "### Crypto Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZcf6aLdX9Hs"
      },
      "outputs": [],
      "source": [
        "list_of_crypto_positive_keyphrase = [\n",
        "         ['سهام','ریزش'],['بورس','ثبات'],['خصوصی','سازی','بورس'],['خصوصی‌سازی','بورس'],['قیمت','افزایش'],['قیمت','رشد'],['پول','رشد'],\n",
        "         ['قیمت','کاهش'],['پول','کاهش'],['قیمت','ریزش'],['پول','ریزش'],['بورس','منفی'],['ریزش','بورس'],\n",
        "         ['متهم','حباب','بورس'],['نابودی','سرمایه'],['دلار','گران'],['دلار','ارزان'],['ارزش','بورس'],['مجوز','اخذ'],\n",
        "         ['مجوز','فعالیت'],['مجوز','بورس'],['بورس','سرمایه','گذاری'],['بورس','سرمایه‌گذاری'],['نوسان','درصد'],\n",
        "         ['نوسان','درصدی'],['سود','واریز'],['سود','پرداخت'],['سود','افزایش'],['سود','کاهش'],['رکود','ارزش','معاملات'],\n",
        "         ['صندوق','سرمایه','بازدهی'],['بازار','سرمایه','تعطیل'],['لغو','عرضه','سهام'],['قرارداد','معامله'],['رشد','نرخ'],\n",
        "         ['تصویب','آیین','نامه','بورس'],['تصویب','آیین‌نامه','بورس'],['راه','اندازی','بورس','بین','الملل'],['راه','اندازی','بورس','بین‌المللی'],\n",
        "         ['ثبت','سفارش','ارز'],['پرداخت','مالیات','معاف'],['توافق','بازخرید'],['نرخ','کاهش'],['نرخ','افزایش'],['آغاز','معاملات'],\n",
        "         ['عرضه','اولیه','سهام'],['عرضه','اولیه','بورس'],['سود','سهامداران'],['سود','سهام','داران'],['سود','سهام‌داران'],['واریز','سود'],\n",
        "         ['ارزش','معاملات','بورس'],['ارزش','معاملات','فرابورس'],['رشد','شاخص'],['افت','شاخص'],['مجموع','معاملات','بورس'],\n",
        "         ['مجموع','معاملات','فرابورس'],['واگذاری','سهام'],['نرخ','بهره'],['قیمت','دلار'],['جذب','سرمایه'],['بازدهی','بیت','کوین'],\n",
        "         ['تعداد','خریداران','کاهش'],['تعداد','خریداران','افزایش'],['راه','اندازی','ارز','دیجیتال'],['راه‌اندازی','ارز','دیجیتال'],['نظارت','ارز','دیجیتال'],\n",
        "         ['ماینر','خریداری'],['بها','ریزش'],['بیت','کوین','سقوط'],['پرداخت','ارز','دیجیتال'],['پذیرش','ارز','قانونی'],['استخراج','پایان'],\n",
        "         ['درآمد','افزایش'],['صرافی','خارج','دلار'],['دلار','خریداری'],['استخراج','کنندگان','درآمد'],['ارزش','بالاترین','سطح'],['حذف','بازار'],\n",
        "         ['برداشت','تعلیق'],['تبلیغ','ارز','دیجیتال','ممنوع'],['بها','سقوط'],['بیت','کوین','خریداری'],['ماینر','بیت','کوین','ارسال'],\n",
        "         ['استخر','بحران','نقدینگی'],['پیوستن','شبکه','بیت','کوین'],['قطعی','شبکه'],['هزینه','تراکنش'],['هزینه','تراکنش‌ها'],\n",
        "         ['ممنوعیت','ارز','دیجیتال'],['ممنوعیت','ارزهای','دیجیتال'],['شرکت','کارکنان','اخراج'],['هک','حساب'],['ماینر','مصرف'],\n",
        "         ['ماینرهای','غیر','مجاز'],['ماینرهای','غیر‌مجاز'],['ماینرهای','غیرمجاز'],['صرافی','اسپانسری','لغو'],['ارز','دیجیتال','راه‌اندازی'],\n",
        "         ['ارزهای','دیجیتال','راه‌اندازی'],['استخراج','انرژی','مصرف'],['حجم','معاملات'],['کاهش','ارزش'],['افزایش','ارزش'],['افزایش','نرخ'],\n",
        "         ['کاهش','نرخ'],['راه‌اندازی','صرافی'],['توکن','عرضه'],['پذیرش','بیت','کوین','آغاز'],['بستر','شبکه','راه‌اندازی'],['درآمد','ماینر'],\n",
        "         ['افزایش','معاملات'],['توسعه','کیف','پول','ارزهای','دیجیتال'],['ریسک','تبلیغات','ارزهای','دیجیتال'],['تعطیلی','استخراج'],\n",
        "         ['افزایش','برق','مصرفی'],['درآمد','استخراج'],['افزودن','دارایی‌های','دیجیتال'],['سرمایه','گذاری','ارزهای','دیجیتال'],\n",
        "         ['سرمایه‌گذاری','ارزهای','دیجیتال'],['رشد','ارزهای','دیجیتال'],['رشد','ارز','دیجیتال'],['استخراج','ترک'],['قانون','مالی','دیجیتال','تصویب'],\n",
        "         ['فساد','مالی'],['پول','شویی'],['دور','زدن','تحریم'],['دور','زدن','تحریم‌ها'],['ارز','دیجیتال','مطالبه'],['ارز','دیجیتال','دزدی'],\n",
        "         ['اختلاس'],['افزايش','تعرفه'],['افزایش','سرمایه'],['تشکیل','صف','خرید'],['تشکیل','صف','فروش'],['صدور','موافقت'],\n",
        "         ['صدور','مجوز'],['صادر','مجوز'],['افزایش','درصدی'],['افزایش','برابری'],['کاهش','برابری'],['کاهش','درصدی'],\n",
        "         ['افشای','اطلاعات'],['فاش','شد'],['فاش','کرد'],['تجمع','سهام‌داران'],['تجمع','سهامداران'],['تجمع','مردم'],['اعتراض','سهام‌داران'],\n",
        "         ['اعتراض','سهامداران'],['افزایش','واحد','پولی'],['افزایش','واحدی'],['کاهش','واحدی'],['میلیارد','دلار'],['میلیون','دلار'],\n",
        "         ['هزار','میلیارد'],['ورود','دنیای','متاورس'],['بی','سابقه'],['بی‌سابقه'],['استقبال','بی','نظیر'],['استقبال','بی‌نظیر'],['استقبال','بینظیر'],\n",
        "         ['استقبال','چشمگیر'],['استقبال','چشم‌گیر'],['هجوم','مردم'],['گام','بلند'],['شایعه'],['شایعات'],['استقبال','گسترده'],['ممنوع','شد'],\n",
        "         ['تحریم','کرد'],['تحریم','شد'],['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],\n",
        "         ['اخراج','شد'],['اخراج','کرد'],['تصویب','شد'],['تصویب','کرد'],['تخلف','محرز'],['اختلاس'],['تغییرات','مدیریتی'],['اسرع','وقت'],\n",
        "         ['برخورد','متخلفان'],['اقدامات','قانونی'],['برخورد','جدی'],['برگزاری','نمایشگاه'],['برگزار','نمایشگاه'],['برگزاری','رویداد'],\n",
        "         ['برگزار','رویداد'],['افتتاح'],['تفاهم‌نامه','امضا'],['توافق','کرد'],['اولین','رسمی'],['آغاز','عرضه'],['پایان','عرضه'],['قطعی','برق'],\n",
        "         ['اسرع','وقت'],['امضای','توافق','نامه'],['امضای','توافقنامه'],['امضای','توافق‌نامه'],['ایرادات','امنیتی'],['مشکلات','امنیتی'],['توافقات','متعدد']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0cet-mCXLb9"
      },
      "source": [
        "### Environment Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP452EchYCfc"
      },
      "outputs": [],
      "source": [
        "list_of_environment_positive_keyphrase = [\n",
        "              ['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],\n",
        "              ['برکنار','کرد'],['اخراج','شد'],['اخراج','کرد'],['دریاچه','ارومیه'],['دریاچه','خزر'],\n",
        "              ['تیراندازی','شکارچیان'],['درگیری','شکارچیان'],['تیراندازی','محیط‌بان'],['تیراندازی','محیط','بان'],\n",
        "              ['مجروح','شدن'],['مجروح','شد'],['مجروح','شدند'],['مجروح','کرد'],['اصابت','گلوگه'],\n",
        "              ['شلیک','گلوله'],['لغو','شد'],['لغو','کرد'],['تعطیل','شد'],['تعطیل','شدند'],['تعطیل','کرد'],\n",
        "              ['منتقل','بیماستان'],['انتقال','بیمارستان'],['انتقال','مصدومان'],['انتقال','مصدوم'],['انتقال','مجروحین'],\n",
        "              ['شناسایی','ضاربان'],['تسلیت'],['شهادت'],['آلوده‌ترین'],['مصوبه','کمیته'],['افزایش','شدید'],\n",
        "              ['کاهش','شدید'],['اعلام','کمیته'],['شرایط','خطرناک'],['وضعیت','خطرناک'],['تشکیل','کمیته'],\n",
        "              ['مرگ','یوزپلنگ‌های','ایران'],['مرگ','یوزپلنگ‌'],['تلف','شدن'],['افشای','دلایل'],['تداوم','ریزگرد'],\n",
        "              ['تداوم','آلودگی','هوا'],['افزایش','ریزگردها'],['شاخص','کیفیت','ناسالم'],['متوقف','شد'],['بسته','شد'],\n",
        "              ['شریک','جرم'],['ثبت','رکورد'],['خطر','انقراض'],['نجات','پلنگ'],['برای','اولین','بار'],['محکوم','شد'],\n",
        "              ['درگیری','شکارچیان'],['منقرض','شد'],['بی','سابقه'],['بی‌سابقه'],['شکارچیان','غیرمجاز'],['شکارچی','غیرمجاز'],\n",
        "              ['شکارچیان','غیر','مجاز'],['شکارچی','غیر','مجاز'],['درگیری','شکارچیان'],['تلفات','انسانی'],['شکار','غیرمجاز'],\n",
        "              ['عملیات','دستگیری'],['عملیات','زنده','گیری'],['معضلی','جهانی'],['معضل','جهانی'],['غیرمجاز'],['غیر‌مجاز'],\n",
        "              ['غیر','مجاز'],['بحران'],['برگزاری','کنفرانس'],['آتش‌سوزی‌','جنگل‌ها'],['آتش‌سوزی‌','جنگل‌های'],['عرضه','شد'],\n",
        "              ['رونمایی','کرد'],['رونمایی','شد'],['تشییع','پیکر'],['فوت','کرد'],['فوت','شد'],['درگذشت'],['نجات','نسل'],\n",
        "              ['کم','آبی','جدی'],['بی','آبی','جدی'],['خشکسالی'],['خشک','سالی'],['خشک‌سالی'],['خشک','شدن','دریاچه'],\n",
        "              ['قطع','کرد'],['هدف','گلوله'],['سلاح','جنگی'],['تقدیر','مراسم'],['تهدید','می‌کند'],['تهدید','کرد'],['تیر','خورده'],\n",
        "              ['تظاهرات','بزرگ'],['نجات','داد'],['نایاب'],['کمیاب'],['کشف','گونه'],['رکورد'],['روند','کاهشی'],['روند','افزایشی'],\n",
        "              ['آلودگی','اقیانوس'],['ممنوع','شد'],['شیوع','تب'],['معرض','خطر'],['بقای','گونه'],['آتش‌سوزی','جنگلی'],['افتتاح'],\n",
        "              ['شناسایی','شد'],['آلودگی','بنادر'],['طوفان'],['وقوع','سیل'],['شکستن','سد'],['هشدار','مقام‌های'],['کشف','ابتلای'],\n",
        "              ['تعقیب','گریز'],['دستگیری','عوامل'],['انقراض'],['هشدار','جدی'],['بازداشت','شدند'],['بازداشت','کردند'],['بازداشت','شد'],\n",
        "              ['بازداشت','کردند'],['سانحه'],['ثبت','ملی'],['حمله','سلاح'],['کشف','شد'],['کشف','کرد'],['خورد','تیر']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3HWWCU5XLfU"
      },
      "source": [
        "### Industry Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxOu6ItEYE-s"
      },
      "outputs": [],
      "source": [
        "list_of_industry_positive_keyphrase = [\n",
        "             ['وزیر','صمت'],['توافق','کشور','خارجی'],['سرمایه','گذاری'],['افزایش','صادرات'],['میلیارد','دلار'],['میلیون','دلار'],\n",
        "             ['میلیارد','تومان'],['میلیون','تومان'],['افزایش','عرضه'],['کاهش','عرضه'],['افزایش','تقاضا'],['کاهش','تقاضا'],\n",
        "             ['افزایش','قیمت'],['کاهش','قیمت'],['گران','کرد'],['گران','شد'],['ارزان','شد'],['ارزان','کرد'],['صدور','مجوز'],\n",
        "             ['صادر','مجور'],['سامانه','مجوز'],['قاچاق','کالا'],['قاچاق','کالای'],['ممنوع','شد'],['تحریم','کرد'],['تحریم','شد'],\n",
        "             ['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],['اخراج','شد'],['اخراج','کرد'],\n",
        "             ['تصویب','شد'],['تصویب','کرد'],['تخلف','محرز'],['اختلاس'],['تغییرات','مدیریتی'],['اسرع','وقت'],['برخورد','متخلفان'],\n",
        "             ['اقدامات','قانونی'],['برخورد','جدی'],['برگزاری','نمایشگاه'],['برگزار','نمایشگاه'],['برگزاری','رویداد'],['برگزار','رویداد'],\n",
        "             ['افتتاح'],['تفاهم‌نامه','امضا'],['توافق','کرد'],['اولین','رسمی'],['آغاز','عرضه'],['پایان','عرضه'],['قطعی','برق'],['حاشیه','اجلاس'],\n",
        "             ['تکذیب','کرد'],['تکذیب','شد'],['افزایش','برابر'],['کاهش','برابر'],['رشد','درصد'],['کاهش','یافت'],['افزایش','یافت'],['حادثه'],['حوادث'],\n",
        "             ['اسرع','وقت'],['امضای','توافق','نامه'],['امضای','توافقنامه'],['امضای','توافق‌نامه'],['ایرادات','امنیتی'],['مشکلات','امنیتی'],['توافقات','متعدد'],\n",
        "             ['محموله','صادراتی'],['محموله','وارداتی'],['مصوب','شد'],['مصوب','کرد'],['مصوب','شده'],['نخستین','بار'],['اولین','بار'],['لغو','کرد'],\n",
        "             ['لغو','شد'],['لغو','شده'],['رفع','مشکلات'],['حذف','قرعه‌کشی','خودرو'],['برگزاری','قرعه‌کشی','خودرو'],['آغاز','پیش','فروش'],['آغاز','پیش‌فروش'],\n",
        "             ['معرفی','کرد'],['معرفی','شد'],['عرضه','کرد'],['عرضه','شد'],['عرضه','می‌شود'],['عرضه','میشود'],['رونمایی','کرد'],['رونمایی','شد'],\n",
        "             ['تفاهم‌نامه','همکاری'],['اعلام','رسمی'],['رسما','اعلام'],['میلیون','دلاری'],['میلیون','دلاری'],['پایان','تعطیلی'],['لغو','تحریم'],['رفع','موانع'],\n",
        "             ['افزایش','تولید'],['رشد','تولید'],['احداث','کند'],['احداث','کرد'],['احداث','شد'],['کشف','معدن'],['انفجار','معدن'],['نام‌گذاری','شد'],\n",
        "             ['نام‌گذاری','کرد'],['آغاز','نام','نویسی'],['آغاز','نام‌نویسی'],['افزایش','درصدی'],['کاهش','درصدی'],['راه‌اندازی','سامانه'],['خودکفا','تولید'],\n",
        "             ['خودکفایی'],['اعلام','نتایج'],['عذرخواهی'],['توقف','تولید'],['پایان','تولید'],['مراسم','افتتاحیه'],['مراسم','اختتامیه'],['رفع','اختلالات'],\n",
        "             ['مقابله','قاچاق'],['افزایش','شدید'],['کاهش','شدید'],['پلمپ','شد'],['پلمپ','کرد'],['آتش','سوزی'],['آتش‌سوزی'],['حریق','دچار']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbFnqfZyXLia"
      },
      "source": [
        "### Social Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyVswpXjYHNk"
      },
      "outputs": [],
      "source": [
        "list_of_social_positive_keyphrase = [\n",
        "             ['ابراز','نگرانی'],['برگزاری','مراسم','بزرگداشت'],['نرخ','باروری','منفی'],['نرخ','رشد','جمعیت'],['کشتار','دسته','جمعی'],\n",
        "             ['اعتراض','دسته','جمعی'],['مرگ','دسته','جمعی'],['حمایت','مردم'],['دستگیر','مجرم'],['هشدار','داد'],['سرکوب','معترضان'],\n",
        "             ['دستگیری','مجرم'],['حمایت','مردمی'],['تجمع','مردم'],['متناسب','سازی','حقوق'],['متناسب‌سازی','حقوق'],['اعتراض','گرانی'],\n",
        "             ['اعتراض','مردم'],['نارضایتی','مردم'],['مردم','ناراضی'],['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],\n",
        "             ['برکنار','شد'],['برکنار','کرد'],['اخراج','شد'],['اخراج','کرد'],['لغو','شد'],['لغو','کرد'],['برگزاری','مراسم'],['برگزار','مراسم'],\n",
        "             ['برگزار','شد'],['برگزار','کرد'],['بسته','شدن','مرز'],['تحمل','نخواهد','کرد'],['افزایش','مستمری'],['کاهش','مستمری'],['کاهش','قدرت','خرید'],\n",
        "             ['افزایش','قدرت','خرید'],['جریان','فتنه'],['واریز','کمک','معیشتی'],['ثبت','اعتراض'],['ابهام','اعتراض'],['برگزاری','کمیته'],['صدور','مجوز'','],\n",
        "             ['صدور','گواهی'],['تصدی','مسئولیت'],['اعلام','استعفا'],['اعلام','استعفای'],['استعفا','کرد'],['استعفا','داد'],['سرسام','آوری'],['سرسام','آور'],\n",
        "             ['معرفی','شد'],['معرفی','کرد'],['تحسین','برانگیزی'],['تحسین','برانگیز'],['شایعه'],['شایعات'],['قطع','کرد'],['هدف','گلوله'],['سلاح','جنگی'],\n",
        "             ['تقدیر','مراسم'],['تهدید','می‌کند'],['تهدید','کرد'],['تیر','خورده'],['تظاهرات','بزرگ'],['نجات','داد'],['هشدار','مقام‌های'],['تعقیب','گریز'],\n",
        "             ['دستگیری','عوامل'],['درگیری','مسلحانه'],['افزایش','دزدی'],['افزایش','جمعیت'],['کاهش','جمعیت'],['افتتاح'],['افزایش','حقوق'],['تظاهرات','مردم'],\n",
        "             ['تظاهرات','مردمی'],['افزایش','قیمت'],['کاهش','قیمت'],['دغدغه','بازنشستگان'],['نرخ','رسمی','تورم'],['تحت','پوشش','بیمه'],['افزایش','جمعیت'],\n",
        "             ['کاهش','جمعیت'],['اعتراض','شدید'],['تجمع','جمعی'],['کاهش','رفاه','اجتماعی'],['افزایش','رفاه','اجتماعی'],['کاهش','رفاه','مردم'],['افزایش','رفاه','مردم'],\n",
        "             ['واکنش','مردم'],['واکنش','شدید'],['کاهش','سطح'],['افزایش','سطح'],['بازتاب','گسترده'],['تسهیلات','ویژه'],['برداشت','جیب','مردم'],\n",
        "             ['برداشت','جیب','شهروندان'],['افزایش','متقاضی'],['هزار','نفر','متقاضی'],['حضور','گسترده'],['فرصت','تمدید','شد'],['صدور','حکم'],\n",
        "             ['قوانین','سخت‌گیرانه'],['قوانین','سخت‌','گیرانه'],['نابودی','معیشت'],['مصدومیت','نفر'],['حکم','قصاص'],['پرونده','تعیین','تکلیف'],['قصاص','قاتل'],\n",
        "             ['رونمایی'],['ایجاد','فاجعه'],['سرقت','خشن'],['اتهامات','گسترده'],['آزادی','زندانی'],['آزادی','زندانیان'],['آزاد','کردن','زندانی'],['گرفتن','حق','مردم'],\n",
        "             ['وصول','حق','مردم'],['تمدید','شد'],['تمدید','کرد'],['مهاجرت','نخبگان'],['حمایت','فعالان'],['قیمت','نجومی'],['قیمت‌های','نجومی'],['کاهش','تعداد','بیماران'],\n",
        "             ['افزایش','تعداد','بیماران'],['افزایش','فوتی'],['کاهش','شدید'],['افزایش','شدید'],['افزایش','تقاضا'],['کاهش','تقاضا'],['محکوم','اعدام'],['جبران','کمبود'],\n",
        "             ['حکم','تخلیه'],['شهادت'],['محل','حادثه'],['خروج','مرز'],['خروج','مرزها'],['سارقان','مسلح'],['طرح','مصوب'],['برگزاری','انتخابات'],\n",
        "             ['حضور','گسترده'],['بی','سابقه'],['بی‌سابقه'],['استقبال','گسترده'],['استقبال','مردم'],['زورگیری','بی‌رحمانه'],['زورگیری','مسلحانه'],['افزایش','هزینه'],\n",
        "             ['کاهش','هزینه'],['طرح','جایگزین'],['اقدام','فوری'],['صدور','حکم'],['صادر','حکم'],['تشکیل','ستاد'],['ستاد','مردمی'],['دستگیر','شد'],\n",
        "             ['دستگیر','کرد'],['دستگیر','شدند'],['زورگیری','خشن'],['زورگیری','مسلحانه'],['زورگیر','خشن'],['دستگیری','زورگیر'],['بازتاب','گسترده'],\n",
        "             ['ترافیک','سنگین'],['تجمع','سنگین'],['ثبت‌نام','متقاضیان'],['ثبت‌','نام','متقاضیان'],['ثبت','نام','متقاضی'],['بهره‌برداری'],['بهره','‌برداری'],\n",
        "             ['مطالبه','مردمی'],['مطالبه','عمومی'],['تجمع','مردمی'],['آدم','ربایی'],['سرقت','خشن'],['ورود','سازمان','بازرسی'],['اعطای','تسهیلات'],\n",
        "             ['طرح','ویژه'],['مصرف','مواد','مخدر'],['مقابله','مواد','مخدر'],['فرار','مالیات'],['فرار','مالیاتی'],['تخلف','آزمون'],['افزایش','درصدی'],\n",
        "             ['کاهش','درصدی'],['اعزام','هزار'],['مطالبه','مردم'],['امضای','تفاهم','نامه'],['امضای','تفاهم‌نامه'],['اعطای','آزادی','مشروط'],['اعطای','تسهیلات'],\n",
        "             ['ورود','سازمان','بازرسی'],['افزایش','نرخ'],['کاهش','نرخ'],['بحرانی'],['اهدای','بسته'],['اهدا','کرد'],['اهدا','شد'],['وقف','مردمی'],\n",
        "             ['وقف','مردم'],['وقف','کرد'],['وقف','شد'],['توقیف','اموال'],['هزار','واحد','مسکونی'],['اختلال','سامانه'],['هشدار','پلیس'],['راه','اندازی'],\n",
        "             ['افزایش','تورم'],['کاهش','تورم'],['اجرا','طرح'],['تخصیص','بودجه'],['اختلاف','تورم'],['اقدامات','دولت'],['رفع','مشکل','مردم'],\n",
        "             ['رفع','کمبود','مردم'],['دستگیری','مجرمان'],['فیلتر','شد'],['فیلتر','کرد'],['اختلال','سامانه'],['پیگیری','مردم'],['برخورد','جدی'],\n",
        "             ['برخورد','بد'],['مبادله','زندانی'],['سانحه','تلخ']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hzQXpUWXLln"
      },
      "source": [
        "### Politics Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pHsimmmYI5s"
      },
      "outputs": [],
      "source": [
        "list_of_politics_positive_keyphrase = [\n",
        "           ['رفع','دغدغه‌های','مردم'],['افزایش','مبادلات','خارجی'],['زندانی','سیاسی'],['تراز','تجاری','مثبت'],\n",
        "           ['حل','مشکلات','مردم'],['رفع','مشکلات','مردم'],['پیگیری','فساد'],['رسیدگی','فساد'],['مانع','واردات','واکسن'],\n",
        "           ['تحریم','شدید'],['توقف','مذاکرات'],['متوقف','مذاکرات'],['خنثی‌سازی','تحریم‌ها'],['رفع','تحریم'],['رفع','تحریم‌ها'],\n",
        "           ['لغو','تحریم‌ها'],['لغو','تحریم'],['خطوط','قرمز'],['خط','قرمز'],['برطرف','موانع','واردات'],['برطرف','موانع','صادرات'],\n",
        "           ['نارضایتی','مردم'],['مردم','ناراضی'],['تجمع','مردم'],['اعتراض','گرانی'],['اعتراض','مردم'],['نارضایتی','مردم'],\n",
        "           ['مردم','ناراضی'],['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],\n",
        "           ['اخراج','شد'],['اخراج','کرد'],['لغو','شد'],['لغو','کرد'],['برگزاری','مراسم'],['برگزار','مراسم'],['برگزار','شد'],\n",
        "           ['برگزار','کرد'],['فشار','حداکثری'],['محکوم','کرذ'],['تصرف','شده'],['موظفند'],['آزادسازی','مناطق'],['مذاکرات','جدی'],\n",
        "           ['دستیابی','توافق'],['سانحه','تلخ'],['سانحه','ناگوار'],['تسلیت'','],['ضایعه','بزرگ'],['شهادت'],['ضایعه','تلخ'],\n",
        "           ['موظف','نظارت'],['موظف','پیگیری'],['موظف','رسیدگی'],['رفع','مشکلات','مردم'],['عفو','بازداشتی'],['گرانی','بنزین'],\n",
        "           ['موانع','اصلی'],['برخورد','مفسد'],['مشکل','امنیتی'],['اختلال','سایت'],['اختلال','سامانه'],['حمله','امنیتی'],['ترور'],\n",
        "           ['میلیون','دلار'],['میلیارد','دلار'],['اختلاس'],['هزار','میلیارد'],['جنایات','تروریستی'],['اقدامات','تحریک','آمیز'],['امضای','سند'],\n",
        "           ['امضا','سند'],['امضای','تفاهم‌نامه'],['امضای','تفاهم‌','نامه'],['امضا','تفاهم‌نامه'],['امضا','تفاهم‌','نامه'],['اجلاس','جهانی'],\n",
        "           ['مجمع','جهانی'],['دارای','اهداف','سیاسی'],['بازداشت','کرد'],['بازداشت','شد'],['منتقل','زندان'],['انتقال','زندان'],\n",
        "           ['محکوم','کرد'],['محکوم','شد'],['محکم','حبس'],['دستگیری','اعتراضات'],['نارضایتی','مردم'],['بحران'],['بحرانی'],\n",
        "           ['شایعه'],['شایعات'],['اعلام','رسمی'],['رسما','اعلام'','],['انتشار','بیانیه‌ای'],['انتشار','بیانیه‌'],['منتشر','بیانیه'],\n",
        "           ['اعلام','بیانیه'],['ورود','مجلس'],['فیلتر','کرد'],['فیلتر','شد'],['محدودیت','مصوبات'],['بازرسی','پرونده'],['بررسی','پرونده'],\n",
        "           ['علنی','اسناد'],['افشای','مدارک'],['افشای','اسناد'],['فاش','شد'],['فاش','کرد'],['دستور','صادر'],['صدور','دستور'],\n",
        "           ['محض','اطلاع'],['دستور','رییس','جمهور'],['دستور','رییس‌جمهور'],['دستور','رهبر'],['دستور','رئیس‌جمهور'],['دستور','رئیس‌','جمهور'],\n",
        "           ['شرکت','مراسم'],['توافق','کرد'],['امضای','توافق'],['امضا','توافق'],['امضای','توافق','نامه'],['امضای','توافق‌نامه'],\n",
        "           ['متهم','کرد'],['متهم','شد'],['مشکل','دولت'],['استیضاح','وزیر'],['تاکید','اجرای','طرح'],['تاکید','اجرای','طرحی'],\n",
        "           ['اجرا','کردن','طرح'],['تحت','حمایت','حکومت','ایران'],['دستگیری','جاسوس'],['جاسوسی','سایبری'],['حملات','سایبری'],\n",
        "           ['انتقال','جاسوس'],['شناسایی','جاسوس'],['دیدار','رئیس','جمهور'],['دیدار','رئیس‌جمهور'],['دیدار','رییس','جمهور'],\n",
        "           ['دیدار','رییس‌جمهور'],['ملاقات','رئیس','جمهور'],['ملاقات','رئیس‌جمهور'],['ملاقات','رییس','جمهور'],['ملاقات','رییس‌جمهور'],\n",
        "           ['انتقال','قدرت'],['برگزاری','جسله'],['برگزار','جسله'],['برگزاری','کمیته'],['برگزار','کمیته'],['عزل','شده'],['عزل','کرد'],\n",
        "           ['عزل','شد'],['عزل','مدیر','ارشد'],['انحلال','مجلس'],['حمله','ترورسیتی'],['حمله','مرگبار'],['جنایت'],['دستگیری','مجرم'],\n",
        "           ['ضد','جمهوری','اسلامی','ایران'],['ضد','ایران'],['هشدار','داد'],['حوادث','اضطراری'],['رایزنی','فوری'],['خبر','فوری'],\n",
        "           ['سکوت','جایز','نیست'],['تداوم','درگیری'],['آسیب','نیروگاه','اتمی'],['حمله','اتمی'],['محدودیت','همه','جانبه'],['محدودیت','همه‌جانبه'],\n",
        "           ['حکم','قضایی'],['دستور','مستقیم'],['سخنان','تحریک‌آمیز'],['شکایت','دولت'],['خرید','مهمات','نظامی'],['تسهیلات','ویژه'],['افتتاح'],\n",
        "           ['حمله','موشکی'],['عامل','انتحاری'],['جنجالی'],['احیای','برجام'],['لغو','مذاکرات'],['هشدار','آژانس','انرژی','اتمی'],\n",
        "           ['واکنش','وزارت','خارجه'],['منتقل','زندانی','سیاسی'],['انتقال','زندانی','سیاسی'],['قطع','روابط'],['سرسام‌آور'],['سرسام','‌آور'],\n",
        "           ['سرسام‌آوری'],['سرسام‌','آوری'],['افت','ارزش'],['افزایش','صادرات'],['کاهش','صادرات'],['افزایش','واردات'],['تشدید','فشار'],\n",
        "           ['فشار','شدید'],['تحقیق','تفحص','ستاد'],['تشکیل','کمیته'],['حمله','هوایی'],['افزایش','تنش'],['تنش','شدید'],['تحت','فشار'],\n",
        "           ['سرکوب','معترضان'],['فساد','مالی'],['توافق','احتمالی'],['نخست','وزیر','جدید'],['استعفا','کرد'],['استعفا','داد'],['استعفای','وزیر'],\n",
        "           ['صدور','حکم'],['دستگیری','مظنون'],['دستگیری','مظنونین'],['متهم','ردیف','اول'],['انتشار','اتهامات'],['اتهامات','منتشر'],['ابلاغ','کرد'],\n",
        "           ['ابلاغ','شد'],['کشتار','دسته','جمعی'],['افزایش','فشار'],['حمله','انتحاری'],['معرفی','شد'],['معرفی','کرد'],['اتهام','جاسوسی'],\n",
        "           ['صدور','حکم','اعدام'],['اعدام','محکوم'],['اعلام','نتیجه','انتخابات'],['برخورد','جدی'],['برخورد','بد'],['پرتاب','فضاپیما'],\n",
        "           ['پرتاب','فضاپیمای'],['تحرک','مشکوک'],['ابراز','نگرانی']                       \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iquYqQDuYQ90"
      },
      "source": [
        "### Game Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FNIDkxUYQ91"
      },
      "outputs": [],
      "source": [
        "list_of_game_positive_keyphrase = [\n",
        "          ['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],['اخراج','شد'],['اخراج','کرد'],\n",
        "          ['سرسام','آوری'],['سرسام','آور'],['معرفی','شد'],['معرفی','کرد'],['تحسین','برانگیزی'],['تحسین','برانگیز'],['معرفی','نسل','جدید'],\n",
        "          ['بهترین','سال'],['اهدای','جوایز'],['اهدا','کرد'],['پشتیبانی','میکند'],['پشتیبانی','می‌کند'],['پشتیبانی','میکنند'],['پشتیبانی','می‌کنند'],\n",
        "          ['نسخه','فروش'],['رکورد','فروش'],['میلیون','کاربر'],['میلیون','فروش'],['میلیارد','فروش'],['به‌روزرسانی'],['پشتیبانی','کرد'],\n",
        "          ['منتشر','شد'],['منتشر','کرد'],['منتشر','میشود'],['منتشر','می‌شود'],['پرفروش‌ترین‌'],['افزایش','قیمت'],['افزایش','تقاضا'],\n",
        "          ['کاهش','قیمت'],['کاهش','تقاضا'],['رسمی'],['لو','رفت'],['خرید','شرکت'],['تاریخ','انتشار'],['تعطیل','شد'],['بهترین','نسخه'],\n",
        "          ['آپدیت','بزرگ'],['آپدیت','بزرگی'],['اپدیت','بزرگ'],['رونمایی'],['تایید','کرد'],['افت','سرعت','اینترنت'],['کاهش','سرعت','اینترنت'],\n",
        "          ['متوقف','شد'],['متوقف','کرد'],['اعتراض'],['تاخیر','عرضه'],['رسما','اعلام'],['هزینه','میلیون'],['هزینه','میلیارد'],['تهدید','کرد'],\n",
        "          ['اولین','تصاویر'],['فوت','کرد'],['فوت','شد'],['دست','ساخت'],['پیشتازی'],['پیشتاز'],['ترک','کرد'],['شایعه'],['برگزاری','رویداد'],\n",
        "          ['برگزار','رویداد'],['عرضه','کرد'],['عرضه','شد'],['عرضه','می‌شود'],['عرضه','میشود'],['زمان','معرفی'],['تاریخ','معرفی'],\n",
        "          ['تاخیر','خورد'],['حالت','تعلیق'],['حذف','شد'],['حذف','شدند'],['حذف','کرد'],['انتشار','بیانیه‌'],['انتشار','بیانیه‌ای'],['خرید','کمپانی'],\n",
        "          ['رفع','باگ'],['مشکلات','امنیتی'],['اختلال','سرور'],['مالکیت','دست','داد'],['رفع','مشکلات'],['فاش','شد'],['فاش','کرد'],\n",
        "          ['فاش','شدند'],['عذرخواهی','کرد'],['عذرخواهی','کردند'],['شایعات'],['افشای','اطلاعات'],['موفقیت','خیره‌کننده‌'],['گران','شد'],\n",
        "          ['گران','کرد'],['ارزان','شد'],['ارزان','کرد'],['موفقیت','غیرمنتظره‌'],['تحریم','کرد'],['تحریم','شد'],['خرید','استودیو'],\n",
        "          ['فروش','استودیو'],['تصاحب','کرد'],['میلیارد','دلار'],['میلیون','دلار'],['اولین','رسمی'],['موجب','خشم'],\n",
        "          ['پایان','پشتیبانی'],['سقوط','ارزش','سهام'],['صعود','ارزش','سهام'],['افزایش','ارزش','سهام'],['کاهش','ارزش','سهام'],\n",
        "          ['میلیارد','نفر'],['میلیون','نفر'],['پایان','سلطه‌'],['تاخیر','خورده']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFEdaTPFW5bc"
      },
      "source": [
        "### Celebrity Key-Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5tWO-qTjclb"
      },
      "outputs": [],
      "source": [
        "list_of_celebrity_positive_keyphrase = [\n",
        "                      ['منصوب','کرد'],['منصوب','شد'],['منصوب','شدند'],['انتصاب'],['برکنار','شد'],['برکنار','کرد'],['اخراج','شد'],\n",
        "                      ['اخراج','کرد'],['دریاچه','ارومیه'],['لغو','شد'],['لغو','کرد'],['تعطیل','شد'],['تعطیل','شدند'],['تعطیل','کرد'],\n",
        "                      ['منتقل','بیماستان'],['انتقال','بیمارستان'],['انتقال','مصدومان'],['انتقال','مصدوم'],['انتقال','مجروحین'],['شناسایی','ضاربان'],\n",
        "                      ['تسلیت'],['شهادت'],['مصوبه','کمیته'],['افزایش','شدید'],['کاهش','شدید'],['اعلام','کمیته'],['شرایط','خطرناک'],\n",
        "                      ['وضعیت','خطرناک'],['تشکیل','کمیته‌'],['افشای','دلایل'],['متوقف','شد'],['بسته','شد'],['شریک','جرم'],\n",
        "                      ['ثبت','رکورد'],['خطر','انقراض'],['برای','اولین','بار'],['محکوم','شد'],['بی','سابقه'],['بی‌سابقه'],['تلفات','انسانی'],\n",
        "                      ['معضلی','جهانی'],['معضل','جهانی'],['غیرمجاز'],['غیر‌مجاز'],['غیر','مجاز'],['بحران'],['برگزاری','کنفرانس'],['عرضه','شد'],\n",
        "                      ['رونمایی','کرد'],['رونمایی','شد'],['تشییع','پیکر'],['فوت','کرد'],['فوت','شد'],['درگذشت'],['نجات','نسل'],['هدف','گلوله'],\n",
        "                      ['سلاح','جنگی'],['تقدیر','مراسم'],['تهدید','می‌کند'],['تهدید','کرد'],['تیر','خورده'],['رکورد'],['ممنوع','شد'],\n",
        "                      ['معرض','خطر'],['افتتاح'],['شناسایی','شد'],['هشدار','مقام‌های'],['تعقیب','گریز'],['دستگیری','عوامل'],['هشدار','جدی'],\n",
        "                      ['بازداشت','شدند'],['بازداشت','کردند'],['بازداشت','شد'],['بازداشت','کردند'],['سانحه'],['حمله','سلاح'],['کشف','شد'],\n",
        "                      ['کشف','کرد'],['مهمانی','شبانه'],['جنجال','سلبریتی'],['جنجالی','سلبریتی'],['چهره','ماندگار'],['سلبریتی','پرحاشیه'],\n",
        "                      ['سلبریتی','پر','حاشیه'],['کشف','حجاب'],['حواشی','مراسم'],['حواشی','شدید'],['مهمانی','جنجالی'],['دورهمی','بازیگران'],\n",
        "                      ['کمیته','انضباطی'],['ممنوع','التصویر'],['ممنوع','تصویر'],['ممنوع','کار'],['ممنوع','الکار'],['مصدومیت','شدید'],['محروم','شد'],\n",
        "                      ['محروم','کرد'],['کمیته','اخلاق'],['حادثه','دلخراش'],['دستگیری','مهمانی'],['بازیگر','معروف'],['فوتبالیست','معروف'],\n",
        "                      ['کشف','مواد','مخدر'],['وضعیت','تاسف','بار'],['تجمع','مردم'],['تغییر','جنسیت'],['حمله','تند'],['درگیری','شدید'],['مهاجرت','کرد'],\n",
        "                      ['برنده','جایزه'],['برنده','جایزه','بهترین'],['ازدحام','عکاسان'],['وایرال','شد'],['عجیب','آنتن','زنده'],['جنجال','برنامه','زنده'],\n",
        "                      ['جنجال','آنتن','زنده'],['حرکت','عجیب','معروف'],['عصبانیت','شدید'],['مورد','توجه','عکاسان'],['استقبال','شدید'],\n",
        "                      ['استقبال','چشمگیر'],['ازدواج','کرد'],['شایعه'],['شایعات'],['طلاق','گرفت'],['گریه','بی','امان'],['سوال','جنجالی'],['مهاجرت','کرد'],\n",
        "                      ['پناهنده','شد'],['توقیف','شد'],['توقیف','کرد'],['توضیح','حواشی','اخیر'],['تشویق','شدید','حضار'],['حمله','شدید'],['سوگواری'],\n",
        "                      ['حمله','تند'],['رونمایی','کرد'],['دریافت','جایزه'],['پوشش','منشوری'],['استوری','خبرساز'],['استوری','جنجالی'],['متاهل','شد'],\n",
        "                      ['کاهش','وزن','زیاد'],['کاهش','وزن','شدید'],['افزایش','وزن','زیاد'],['افزایش','وزن','شدید'],['حرکت','باورنکردنی'],\n",
        "                      ['خوش','گذرونی','لاکچری'],['زندگی','لاکچری'],['افشای','تصاویر'],['افشای','اطلاعات'],['اطلاعات','محرمانه'],['استایل','منشوری'],\n",
        "                      ['لو','رفت'],['لو','رفتن'],['دسترس','خارج','شد'],['تولد','لاکچری'],['خیانت','کرد'],['سوژه','عکاسان'],\n",
        "                      ['مصاحبه','جنجالی'],['حاضر','مصاحبه','نشد'],['پوشش','نامتعارف'],['دستمزد','نجومی'],['دستمزدهای','نجومی'],['هشدار','پلیس'],['فرار','مالیاتی'],\n",
        "                      ['صادر','حکم'],['صادر','رای'],['انتقاد','تند'],['واکنش','عجیب'],['واکنش','جنجالی'],['پرداخت','غرامت']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj0xvKy5bIY8"
      },
      "source": [
        "## Sub-Functions Needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTCVt2zFT7w4"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "def remove_url(tweet):\n",
        "    return re.sub(r\"http\\S+\", \" \", tweet)\n",
        "\n",
        "def remove_usernames(tweet):\n",
        "    return re.sub('@[^\\s]+',' ',tweet)\n",
        "\n",
        "def remove_English(tweet):\n",
        "  return re.sub(r'\\s*[A-Za-z]+\\b', ' ' , tweet).rstrip()\n",
        "\n",
        "def remove(text):\n",
        "  text = text.replace('ھ','ه')\n",
        "  text = text.replace('ئ','ی')\n",
        "  text = text.replace('ؤ','و')\n",
        "  text = text.replace('إ','ا')\n",
        "  text = text.replace('أ','ا')\n",
        "  text = text.replace('َ','')\n",
        "  text = text.replace('ُ','')\n",
        "  text = text.replace('ِ','')\n",
        "  text = text.replace('ّ','')\n",
        "  text = text.replace('ء','')\n",
        "  text = text.replace('ـ','')\n",
        "  text = text.replace('،',' ')\n",
        "  text = text.replace('،',' ')\n",
        "  text = re.sub('[^\\u0621-\\u0628\\u062A-\\u063A\\u0641-\\u0642\\u0644-\\u0648\\u064E-\\u0651\\u0655\\u067E\\u0686\\u0698\\u06A9\\u06AF\\u06BE\\u06CC\\u06F0-\\u06F9 ]', ' ', text)\n",
        "  return re.sub(\"[^0-9\\u0600-\\u06FF]+\", \" \", text).strip()\n",
        "\n",
        "def control_numeric(text):\n",
        "  text = text\n",
        "  pattern_integer = r'\\d+'\n",
        "  pattern_float = \"\\d+\\.\\d+\"\n",
        "  text = re.sub(pattern_float, ' ', text)\n",
        "  text = re.sub(pattern_integer, ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_nan(dataset,column):\n",
        "  dataset = dataset[dataset[column].notna()]\n",
        "  return dataset\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "  tweet = remove(tweet)\n",
        "  tweet = remove_emoji(tweet)\n",
        "  tweet = remove_usernames(tweet)\n",
        "  tweet = remove_url(tweet)\n",
        "  tweet = remove_English(tweet)\n",
        "  tweet = control_numeric(tweet)\n",
        "  return Normalizer().normalize(tweet)\n",
        "\n",
        "def tweet_to_token(tweet):\n",
        "    return word_tokenize(tweet)\n",
        "\n",
        "def remove_StopWords(text):\n",
        "  return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def calculate_max_length(tweet_array):\n",
        "  max_length = 0\n",
        "  for tweet in tweet_array:\n",
        "    if max_length < len(tweet):\n",
        "      max_length = len(tweet)\n",
        "  return max_length\n",
        "\n",
        "def Text2KeyPhrase(text):\n",
        "  return kw_model.extract_keywords(text, keyphrase_ngram_range=(4, 7), stop_words=None)\n",
        "\n",
        "def plot_history(history):\n",
        "    \n",
        "    plt.figure(figsize=(8,5),linewidth = 7, edgecolor=\"whitesmoke\")    \n",
        "    n = len(history.history['accuracy'])\n",
        "    \n",
        "    plt.plot(np.arange(0,n)+1,history.history['accuracy'], color='orange',marker=\".\")\n",
        "    plt.plot(np.arange(0,n)+1,history.history['loss'],'b',marker=\".\")\n",
        "    \n",
        "    # offset both validation curves\n",
        "    plt.plot(np.arange(0,n)+ 1,history.history['val_accuracy'],'r')  \n",
        "    plt.plot(np.arange(0,n)+ 1,history.history['val_loss'],'g')\n",
        "    \n",
        "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # set vertical limit to 1\n",
        "    plt.gca().set_ylim(0,1)\n",
        "\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.suptitle(\"Learning Curve\", size=16, y=0.927)\n",
        "    plt.show()\n",
        "\n",
        "def token2lemma(token):\n",
        "  return Lemmatizer().lemmatize(token).split('#')[0]\n",
        "\n",
        "def get_index(tupl, pos):\n",
        "  return [i for i, tupl in enumerate(tupl) if tupl[-1] == pos]\n",
        "\n",
        "def norm_token_persian(sentence):\n",
        "  return word_tokenize(Normalizer().normalize(sentence))\n",
        "\n",
        "def persian_pos(tokens):\n",
        "  tagger = POSTagger(model='postagger.model')\n",
        "  removed_roles = ['POSTP','CONJ','PUNC','NUM']\n",
        "  part_of_speech = tagger.tag(tokens)\n",
        "  remove_indices = list()\n",
        "  for i in range(len(part_of_speech)):\n",
        "    if part_of_speech[i][-1] in removed_roles:\n",
        "      remove_indices.append(i)\n",
        "  part_of_speech = [i for j, i in enumerate(part_of_speech) if j not in remove_indices]\n",
        "  return part_of_speech\n",
        "\n",
        "def persian_lemma_stem(POS):\n",
        "  noun_index = get_index(POS,'N')\n",
        "  # verb_index = get_index(POS,'V')\n",
        "  POS = [*map(list, POS)]\n",
        "  for i in noun_index:\n",
        "    POS[i][0] = Stemmer().stem(POS[i][0])\n",
        "  # for j in verb_index:\n",
        "  #   POS[j][0] = token2lemma(POS[j][0])\n",
        "  lemma_stem = [i[0] for i in POS]\n",
        "  return POS, lemma_stem, ' '.join([word for word in lemma_stem])\n",
        "\n",
        "def count_chars(text):\n",
        "  return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "  return len(text.split())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs1AcBh3bw2S"
      },
      "source": [
        "## DatasetLabeling Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IsiVE2JxLp7"
      },
      "outputs": [],
      "source": [
        "class DatasetLabeling:\n",
        "  \n",
        "  def __init__(self, \n",
        "               data_path,\n",
        "               positive_keyPhrase,\n",
        "               negative_keyPhrase,\n",
        "               spam_keyPhrase):\n",
        "        self.data_path = data_path\n",
        "        self.positive_keyPhrase = positive_keyPhrase\n",
        "        self.negative_keyPhrase = negative_keyPhrase\n",
        "        self.spam_keyPhrase = spam_keyPhrase\n",
        "\n",
        "  def read_data(self):\n",
        "    self.data = pd.read_csv(\n",
        "        self.data_path,sep=',', encoding = \"utf-8-sig\",on_bad_lines='skip')\n",
        "    self.data['text']= self.data['text'].astype(str)\n",
        "    \n",
        "  def clean_text(self):\n",
        "    self.data['text'] = self.data['text'].apply(clean_tweet)\n",
        "\n",
        "\n",
        "  def filter(self,list_of_tokens):\n",
        "    for keyphrase in self.spam_keyPhrase:\n",
        "      if any(token in list_of_tokens for token in keyphrase):\n",
        "        return 'Spam'\n",
        "    for keyphrase in self.positive_keyPhrase:\n",
        "      if all(token in list_of_tokens for token in keyphrase):\n",
        "        return 'Positive'\n",
        "      # for keyphrase in self.negative_keyPhrase:\n",
        "      #   if all(token in list_of_tokens for token in keyphrase):\n",
        "      #     return 'Negative'\n",
        "    return 'Negative'\n",
        "\n",
        "  def detect_labels(self):\n",
        "    list_of_labels = []\n",
        "    for tweets in self.data['text']:\n",
        "      token = norm_token_persian(tweets)\n",
        "      list_of_labels.append(self.filter(token))\n",
        "    return list_of_labels\n",
        "      \n",
        "  def manual_dataset_labeling(self):\n",
        "    labeled_dataset = self.data.copy()\n",
        "    labeled_dataset['Label'] = self.detect_labels()\n",
        "    labeled_dataset.drop(labeled_dataset[labeled_dataset.Label == 'Spam'].index, inplace=True)\n",
        "    labeled_dataset.loc[labeled_dataset['Label'] == 'Positive', ['Label']]= 1\n",
        "    labeled_dataset.loc[labeled_dataset['Label'] == 'Negative', ['Label']]= 0\n",
        "    labeled_dataset.loc[labeled_dataset['Label'] == 'No_Sense', ['Label']]= -1\n",
        "    self.labeled_dataset = labeled_dataset\n",
        "  \n",
        "  def Main(self):\n",
        "    self.read_data()\n",
        "    self.clean_text()\n",
        "    self.manual_dataset_labeling()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0oUl8UXb2Ji"
      },
      "source": [
        "## DataProcessor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGBgxp4VxLkI"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "  \n",
        "  def __init__(self, \n",
        "               Labeled_Dataset,\n",
        "               Pars_Bert_Model,\n",
        "               Down_Sampleing_Factor = 1.5,\n",
        "               Split_Factor = 0.2,\n",
        "               flag=False,\n",
        "               \n",
        "               ):\n",
        "        self.data = Labeled_Dataset\n",
        "        self.Down_Sampleing_Factor = Down_Sampleing_Factor\n",
        "        self.Split_Factor = Split_Factor\n",
        "        self.flag = flag\n",
        "        self.Pars_Bert_Model = Pars_Bert_Model\n",
        "\n",
        "  def down_sampeling(self):\n",
        "    positive = self.data[self.data['Label']==1]\n",
        "    negative = self.data[self.data['Label']==0].sample(n=int(len(positive)*self.Down_Sampleing_Factor))\n",
        "    return pd.concat([positive, negative], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "  def split_data(self):\n",
        "    down_sampeled = self.down_sampeling()\n",
        "    feature , label = down_sampeled.iloc[:,0:] , down_sampeled['Label']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(feature, label, test_size=self.Split_Factor, shuffle=True)\n",
        "    self.train_dataset, self.test_dataset = X_train, X_test\n",
        "    self.train_text = list(map(remove_StopWords, X_train['text']))\n",
        "    self.test_text = list(map(remove_StopWords, X_test['text']))\n",
        "    self.y_train = np.asarray(y_train).astype('int')\n",
        "    self.y_test = np.asarray(y_test).astype('int')\n",
        "\n",
        "  def text2sequence(self):\n",
        "    tokenizer = text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(list(self.train_text))\n",
        "    train_input = tokenizer.texts_to_sequences(self.train_text)\n",
        "    self.X_train = sequence.pad_sequences(train_input, maxlen = calculate_max_length(self.train_text))\n",
        "\n",
        "    test_input = tokenizer.texts_to_sequences(self.test_text)\n",
        "    self.X_test = sequence.pad_sequences(test_input, calculate_max_length(self.X_train)) \n",
        "\n",
        "  def text_summarization(self):\n",
        "    self.X_train_summarized = []\n",
        "    self.Y_train_summarized = []\n",
        "    self.flag = True\n",
        "    i=0\n",
        "    for text in self.train_text:\n",
        "      temp = Text2KeyPhrase(text,20,20)\n",
        "      if len(temp) != 0 :\n",
        "        self.X_train_summarized.append(temp[0][0])\n",
        "        self.Y_train_summarized.append(self.y_train[i])\n",
        "      i += 1\n",
        "    \n",
        "  def Bert_Embedding(self):\n",
        "    if not self.flag:\n",
        "      self.train_embedding = self.Pars_Bert_Model.encode(self.train_text)\n",
        "      self.test_embedding = self.Pars_Bert_Model.encode(self.test_text)\n",
        "    else:\n",
        "      self.train_embedding = self.Pars_Bert_Model.encode(self.X_train_summarized)\n",
        "      self.train_embedding = self.Pars_Bert_Model.encode(self.train_text)\n",
        "\n",
        "  \n",
        "  def Main(self):\n",
        "    self.split_data()\n",
        "    # self.text2sequence()\n",
        "    # self.text_summarization()\n",
        "    # self.Bert_Embedding()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVJGRFiWb8G6"
      },
      "source": [
        "## Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoWAy5cjxLha"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "\n",
        "  def __init__(self, \n",
        "               X_train,\n",
        "               y_train,\n",
        "               X_test,\n",
        "               y_test,\n",
        "               train_embedding,\n",
        "               test_embedding,\n",
        "               train_dataset = [],\n",
        "               test_dataset = [],\n",
        "               batch_size = 128,\n",
        "               n_epochs = 100,\n",
        "               filters = 250,               \n",
        "               kernel_size = 3,\n",
        "               units = 250,\n",
        "               optimizer = tf.optimizers.SGD,\n",
        "               drop_rate = 0.4,\n",
        "               learning_rate = 0.02,\n",
        "               validation_split = 0.10\n",
        "               ):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.train_embedding = train_embedding\n",
        "        self.test_embedding = test_embedding\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.units = units\n",
        "        self.optimizer = optimizer\n",
        "        self.drop_rate = drop_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "  def define_cnn_model(self):\n",
        "      \n",
        "      model = Sequential()\n",
        "      \n",
        "      #create embedding layer\n",
        "      model.add(Embedding(self.train_embedding.shape[0], self.train_embedding.shape[1], trainable = False\n",
        "                        ,embeddings_initializer = tf.keras.initializers.Constant(self.train_embedding)))    \n",
        "      # 1st dropout\n",
        "      model.add(Dropout(self.drop_rate))\n",
        "      \n",
        "      # 1st convolutional 1-D layer\n",
        "      model.add(Conv1D(self.filters, self.kernel_size, padding = 'valid')) #no padding\n",
        "      \n",
        "      #max pooling layer\n",
        "      model.add(MaxPooling1D())\n",
        "\n",
        "      # 2nd convolutional 1-D layer\n",
        "      model.add(Conv1D(self.filters, self.kernel_size, padding = 'valid')) #no padding\n",
        "      \n",
        "      #max pooling layer\n",
        "      model.add(MaxPooling1D())\n",
        "      \n",
        "      # 3rd convolutional 1-D layer\n",
        "      model.add(Conv1D(self.filters, self.kernel_size, padding = 'valid', activation = 'relu'))\n",
        "      \n",
        "      # global max pooling layer\n",
        "      model.add(GlobalAveragePooling1D())\n",
        "      \n",
        "      # 1st dense layer\n",
        "      model.add(Dense(self.units,activation = 'relu'))\n",
        "      \n",
        "      # 2nd dropout\n",
        "      model.add(Dropout(self.drop_rate))\n",
        "      \n",
        "      # final dense layer\n",
        "      model.add(Dense(1,activation = 'sigmoid'))\n",
        "      \n",
        "      # compile the model\n",
        "      model.compile(loss = 'binary_crossentropy',    #since we are doing binary classification\n",
        "                  optimizer = tf.optimizers.SGD(learning_rate = self.learning_rate),\n",
        "                  metrics = ['accuracy']) \n",
        "\n",
        "      self.cnn_model = model\n",
        "    \n",
        "  def train_cnn(self):\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', patience=25, verbose=0, mode='min')\n",
        "    # mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "    # reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta=1e-3, mode='min')\n",
        "\n",
        "    self.cnn_history = self.cnn_model.fit(\n",
        "        self.X_train, self.y_train,\n",
        "        batch_size = self.batch_size,\n",
        "        validation_split = self.validation_split,\n",
        "        callbacks = [earlyStopping],\n",
        "        epochs = self.n_epochs\n",
        "        )\n",
        "    \n",
        "  def plot_cnn_training_history(self):\n",
        "    return plot_history(self.cnn_history)\n",
        "\n",
        "  def Evaluate_cnn(self):\n",
        "    self.cnn_prediction = self.cnn_model.predict(self.X_test, verbose = 1, batch_size = self.batch_size)\n",
        "    prediction = self.cnn_prediction\n",
        "    prediction[prediction >= 0.5] = 1\n",
        "    prediction[prediction < 0.5] = 0\n",
        "    print(classification_report(self.y_test, prediction))\n",
        "    print(confusion_matrix(self.y_test, prediction))\n",
        "      \n",
        "  def save_model(self, model, path):\n",
        "    return pickle.dump(self.model, open(path, 'wb'))\n",
        "\n",
        "  def load_model(path):\n",
        "    return pickle.load(open(path, 'rb'))\n",
        "\n",
        "  def svm_model(self):\n",
        "    self.svm_model = svm.NuSVC(kernel='poly',gamma=\"auto\", probability=False)\n",
        "    self.svm_model.fit(self.train_embedding, self.y_train)\n",
        "  \n",
        "  def Evaluate_svm(self):\n",
        "    self.svm_prediction = self.svm_model.predict(self.test_embedding)\n",
        "    print(classification_report(self.y_test, self.svm_prediction))\n",
        "    print(confusion_matrix(self.y_test, self.svm_prediction))\n",
        "\n",
        "  def naive_bayes_model(self):\n",
        "    nb_train_input = self.train_embedding + abs(np.amin(self.train_embedding))\n",
        "    self.nb_model = MultinomialNB()\n",
        "    self.nb_model.fit(nb_train_input, self.y_train)\n",
        "  \n",
        "  def Evaluate_naive_bayes(self):\n",
        "    nb_test_input = self.test_embedding + abs(np.amin(self.test_embedding))\n",
        "    self.nb_prediction = self.nb_model.predict(nb_test_input)\n",
        "    print(classification_report(self.y_test, self.nb_prediction))\n",
        "    print(confusion_matrix(self.y_test, self.nb_prediction))\n",
        "  \n",
        "  def feature_extraction(self):\n",
        "    scaler = StandardScaler()\n",
        "    self.train_dataset['svm_log_prob_0'] = pd.Series(self.svm_model.predict_proba(self.train_embedding)[::,0])\n",
        "    self.train_dataset['svm_log_prob_1'] = pd.Series(self.svm_model.predict_proba(self.train_embedding)[::,-1])\n",
        "    self.train_dataset['cnn_prob'] = pd.Series(self.cnn_model.predict(self.X_train, verbose = 1, batch_size = self.batch_size).T.ravel())\n",
        "    self.train_dataset['num_char'] = self.train_dataset['text'].apply(count_chars)\n",
        "    self.train_dataset['num_words'] = self.train_dataset['text'].apply(count_words)\n",
        "    self.train_dataset.fillna(value=0, inplace=True)\n",
        "    self.train_features = self.train_dataset[['svm_log_prob_0','svm_log_prob_1','num_words','num_words','like_count','retweet_count']].to_numpy()\n",
        "    self.train_features = scaler.fit_transform(self.train_features)\n",
        "    \n",
        "\n",
        "    self.test_dataset['svm_log_prob_0'] = pd.Series(self.svm_model.predict_proba(self.test_embedding)[::,0])\n",
        "    self.test_dataset['svm_log_prob_1'] = pd.Series(self.svm_model.predict_proba(self.test_embedding)[::,-1])\n",
        "    self.test_dataset['cnn_prob'] = pd.Series(self.cnn_model.predict(self.X_test, verbose = 1, batch_size = self.batch_size).T.ravel())\n",
        "    self.test_dataset['num_char'] = self.test_dataset['text'].apply(count_chars)\n",
        "    self.test_dataset['num_words'] = self.test_dataset['text'].apply(count_words)\n",
        "    self.test_dataset.fillna(value=0, inplace=True)\n",
        "    self.test_features = self.test_dataset[['svm_log_prob_0','svm_log_prob_1','num_words','num_words','like_count','retweet_count']].to_numpy()\n",
        "    self.test_features = scaler.fit_transform(self.test_features)\n",
        "\n",
        "  def final_classifier(self):\n",
        "    self.final_DT = DecisionTreeClassifier().fit(self.train_features, self.y_train)\n",
        "    self.final_DT_prediction = self.final_DT.predict(self.test_features)\n",
        "    print(classification_report(self.y_test, self.final_DT_prediction))\n",
        "    print(confusion_matrix(self.y_test, self.final_DT_prediction))\n",
        "\n",
        "\n",
        "  def Main(self):\n",
        "    # print('----------- Training of SVM Model Starts -----------')\n",
        "    # print()\n",
        "    # self.svm_model()\n",
        "    # print()\n",
        "    # print('----------- Evaluation of SVM Model Starts -----------')\n",
        "    # print()\n",
        "    # self.Evaluate_svm()\n",
        "    # print()\n",
        "    # print('----------- Training of NB Model Starts -----------')\n",
        "    # print()\n",
        "    # self.naive_bayes_model()\n",
        "    # print()\n",
        "    # print('----------- Evaluation of NB Model Starts -----------')\n",
        "    # print()\n",
        "    # self.Evaluate_naive_bayes()\n",
        "    self.define_cnn_model()\n",
        "    print()\n",
        "    print('----------- Training of CNN Model Starts -----------')\n",
        "    print()    \n",
        "    self.train_cnn()\n",
        "    self.plot_cnn_training_history()\n",
        "    print()\n",
        "    print('----------- Evaluation of CNN Model Starts -----------')\n",
        "    print()\n",
        "    self.Evaluate_cnn()\n",
        "    # self.feature_extraction()\n",
        "    # self.final_classifier()\n",
        "    # self.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udh9G3-EcBuK"
      },
      "source": [
        "## Predict Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9LH8yvuwbXq"
      },
      "outputs": [],
      "source": [
        "class Predict:\n",
        "  \n",
        "  def __init__(self,\n",
        "               \n",
        "               SVM_model_tech,\n",
        "               CNN_model_tech,\n",
        "               tech_train_text,\n",
        "\n",
        "               SVM_model_sport,\n",
        "               CNN_model_sport,\n",
        "               sport_train_text,\n",
        "\n",
        "               SVM_model_economy,\n",
        "               CNN_model_economy,\n",
        "               economy_train_text,\n",
        "\n",
        "               SVM_model_art,\n",
        "               CNN_model_art,\n",
        "               art_train_text,\n",
        "\n",
        "               ParsBert\n",
        "               ):\n",
        "        self.SVM_model_tech = SVM_model_tech\n",
        "        self.CNN_model_tech = CNN_model_tech\n",
        "        self.tech_train_text = tech_train_text\n",
        "\n",
        "        self.SVM_model_sport = SVM_model_sport\n",
        "        self.CNN_model_sport = CNN_model_sport\n",
        "        self.sport_train_text = sport_train_text\n",
        "\n",
        "        self.SVM_model_economy = SVM_model_economy\n",
        "        self.CNN_model_economy = CNN_model_economy\n",
        "        self.economy_train_text = economy_train_text\n",
        "\n",
        "        self.SVM_model_art = SVM_model_art\n",
        "        self.CNN_model_art = CNN_model_art\n",
        "        self.art_train_text = art_train_text\n",
        "\n",
        "        self.ParsBert = ParsBert\n",
        "  \n",
        "  def feature_extrction(self, pp_text, category):\n",
        "    text_embedding = self.ParsBert.encode(pp_text)\n",
        "    tokenizer = text.Tokenizer()\n",
        "    if category == 'sport':\n",
        "      tokenizer.fit_on_texts(list(self.sport_train_text))\n",
        "    elif category == 'tech':\n",
        "      tokenizer.fit_on_texts(list(self.tech_train_text))\n",
        "    elif category == 'economy':\n",
        "      tokenizer.fit_on_texts(list(self.economy_train_text))\n",
        "    elif category == 'art':\n",
        "      tokenizer.fit_on_texts(list(self.art_train_text))\n",
        "\n",
        "    text_input = tokenizer.texts_to_sequences([pp_text])\n",
        "    encoded_text = sequence.pad_sequences(text_input, maxlen = len(pp_text))\n",
        "    return text_embedding, encoded_text\n",
        "\n",
        "  def Main(self):\n",
        "    while True:\n",
        "      sample = input('Please Enter your Text (Or Enter Quit): ')\n",
        "      if sample == 'Quit':\n",
        "        break\n",
        "      category = input('Enter Category of Text (sport/ tech/ economy/ art/): ')\n",
        "      \n",
        "      start_time = time.time()\n",
        "      sample = clean_tweet(sample)\n",
        "      text_embedding, encoded_text = self.feature_extrction(sample,category)\n",
        "\n",
        "      if category == 'sport':\n",
        "        cnn_prediction = self.CNN_model_sport.predict(encoded_text, verbose = 1, batch_size = 64)\n",
        "        svm_prediction = self.SVM_model_sport.predict(text_embedding.reshape(-1, 768))\n",
        "        end_time = time.time()\n",
        "        print('input text: ',clean_tweet(sample))\n",
        "        print('CNN model output: ', cnn_prediction)\n",
        "        print('SVM model output: ', svm_prediction)\n",
        "        print('Time Duration:',(end_time - start_time),'Seconds')\n",
        "      \n",
        "      elif category == 'tech':\n",
        "        cnn_prediction = self.CNN_model_tech.predict(encoded_text, verbose = 1, batch_size = 128)\n",
        "        svm_prediction = self.SVM_model_tech.predict(text_embedding.reshape(-1, 768))\n",
        "        end_time = time.time()\n",
        "        print('input text: ',clean_tweet(sample))\n",
        "        print('CNN model output: ', cnn_prediction)\n",
        "        print('SVM model output: ', svm_prediction)\n",
        "        print('Time Duration:',(end_time - start_time),'Seconds')\n",
        "      \n",
        "      elif category == 'economy':\n",
        "        cnn_prediction = self.CNN_model_economy.predict(encoded_text, verbose = 1, batch_size = 128)\n",
        "        svm_prediction = self.SVM_model_economy.predict(text_embedding.reshape(-1, 768))\n",
        "        end_time = time.time()\n",
        "        print('input text: ',clean_tweet(sample))\n",
        "        print('CNN model output: ', cnn_prediction)\n",
        "        print('SVM model output: ', svm_prediction)\n",
        "        print('Time Duration:',(end_time - start_time),'Seconds')\n",
        "      \n",
        "      elif category == 'art':\n",
        "        cnn_prediction = self.CNN_model_art.predict(encoded_text, verbose = 1, batch_size = 128)\n",
        "        svm_prediction = self.SVM_model_art.predict(text_embedding.reshape(-1, 768))\n",
        "        end_time = time.time()\n",
        "        print('input text: ',clean_tweet(sample))\n",
        "        print('CNN model output: ', cnn_prediction)\n",
        "        print('SVM model output: ', svm_prediction)\n",
        "        print('Time Duration:',(end_time - start_time),'Seconds')\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0g9uncWcPjL"
      },
      "source": [
        "## Thechnology Event Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie9iVYUy-r7b",
        "outputId": "953e5bef-68ad-4598-ffae-8cdb126cb6af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        }
      ],
      "source": [
        "Tech_dataset = DatasetLabeling(\n",
        "               '/content/gdrive/MyDrive/Event Detection Final Dataset/Technology.csv',list_of_technology_positive_keyphrase,[],spam_keyphrase)\n",
        "Tech_dataset.Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izhJ-Q7S54KB"
      },
      "outputs": [],
      "source": [
        "Tech_processed = DataProcessor(Tech_dataset.labeled_dataset,\n",
        "                               Pars_Bert_Model,\n",
        "                               Down_Sampleing_Factor=1.5,\n",
        "                               Split_Factor=0.2)\n",
        "Tech_processed.Main()\n",
        "del Tech_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SudE1lfBchFx"
      },
      "source": [
        "## Sport Event Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTxTYyBT3p-S",
        "outputId": "f3016eb3-044f-4f42-dd65-71ad2374a4a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        }
      ],
      "source": [
        "Sport_dataset = DatasetLabeling(\n",
        "               '/content/gdrive/MyDrive/Event Detection Final Dataset/Sport.csv',list_of_sport_positive_keyphrase,[],spam_keyphrase)\n",
        "Sport_dataset.Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NcokyuC73rQb"
      },
      "outputs": [],
      "source": [
        "Sport_processed = DataProcessor(Sport_dataset.labeled_dataset,\n",
        "                                Pars_Bert_Model,\n",
        "                                Down_Sampleing_Factor=1.5,\n",
        "                                Split_Factor=0.2)\n",
        "Sport_processed.Main()\n",
        "del Sport_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVQsPQAQct04"
      },
      "source": [
        "## Economy Event Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OBL7NExs9-eL"
      },
      "outputs": [],
      "source": [
        "Economy_dataset = DatasetLabeling(\n",
        "               '/content/gdrive/MyDrive/Event Detection Final Dataset/Economy.csv',list_of_economy_positive_keyphrase,[],spam_keyphrase)\n",
        "Economy_dataset.Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gAHBigxpY2qw"
      },
      "outputs": [],
      "source": [
        "Economy_processed = DataProcessor(Economy_dataset.labeled_dataset,\n",
        "                                  Pars_Bert_Model,\n",
        "                                  Down_Sampleing_Factor=1.5,\n",
        "                                  Split_Factor=0.2)\n",
        "Economy_processed.Main()\n",
        "del Economy_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3ivE_tuzg6"
      },
      "source": [
        "## Art Event Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AAlI7IWquzg7"
      },
      "outputs": [],
      "source": [
        "Art_dataset = DatasetLabeling(\n",
        "               '/content/gdrive/MyDrive/Event Detection Final Dataset/Art.csv',list_of_art_positive_keyphrase,[],spam_keyphrase)\n",
        "Art_dataset.Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Acmy5UbeY3RO"
      },
      "outputs": [],
      "source": [
        "Art_processed = DataProcessor(Art_dataset.labeled_dataset,\n",
        "                              Pars_Bert_Model,\n",
        "                              Down_Sampleing_Factor=1.5,\n",
        "                              Split_Factor=0.2)\n",
        "Art_processed.Main()\n",
        "del Art_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MM7ZIEzhH9S"
      },
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VBQTZNqUIfE7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7BTShbaIf-z"
      },
      "outputs": [],
      "source": [
        "json_file = open('/content/gdrive/MyDrive/Event_Detection_Models/Technology_cnn_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "CNN_model_tech = model_from_json(loaded_model_json)\n",
        "CNN_model_tech.load_weights(\"/content/gdrive/MyDrive/Event_Detection_Models/Technology_cnn_model.h5\")\n",
        "SVM_model_tech = pickle.load(open('/content/gdrive/MyDrive/Event_Detection_Models/Technology_svm_model', 'rb'))\n",
        "print(\"Model Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Y7rrpUsFAi8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a82c0a-4c41-4003-922c-50e6f7bf9fb2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded\n"
          ]
        }
      ],
      "source": [
        "json_file = open('/content/gdrive/MyDrive/Event_Detection_Models/Sport_cnn_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "CNN_model_sport = model_from_json(loaded_model_json)\n",
        "CNN_model_sport.load_weights(\"/content/gdrive/MyDrive/Event_Detection_Models/Sport_cnn_model.h5\")\n",
        "SVM_model_sport = pickle.load(open('/content/gdrive/MyDrive/Event_Detection_Models/Sport_svm_model', 'rb'))\n",
        "print(\"Model Loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UT7URgFhIkgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea2a0b3-9284-46f1-897a-ab06c94ef5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded\n"
          ]
        }
      ],
      "source": [
        "json_file = open('/content/gdrive/MyDrive/Event_Detection_Models/Economy_cnn_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "CNN_model_economy = model_from_json(loaded_model_json)\n",
        "CNN_model_economy.load_weights(\"/content/gdrive/MyDrive/Event_Detection_Models/Economy_cnn_model.h5\")\n",
        "SVM_model_economy = pickle.load(open('/content/gdrive/MyDrive/Event_Detection_Models/Economy_svm_model', 'rb'))\n",
        "print(\"Model Loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg3NajpIIkgv"
      },
      "outputs": [],
      "source": [
        "json_file = open('/content/gdrive/MyDrive/Event_Detection_Models/Art_cnn_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "CNN_model_art = model_from_json(loaded_model_json)\n",
        "CNN_model_art.load_weights(\"/content/gdrive/MyDrive/Event_Detection_Models/Art_cnn_model.h5\")\n",
        "SVM_model_art = pickle.load(open('/content/gdrive/MyDrive/Event_Detection_Models/Art_svm_model', 'rb'))\n",
        "print(\"Model Loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMd7AoGxcmnQ"
      },
      "source": [
        "## Real-Time Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZxKYo15xKTZo"
      },
      "outputs": [],
      "source": [
        "Real_time_Prediction = Predict(\n",
        "               SVM_model_tech,\n",
        "               CNN_model_tech,\n",
        "               Tech_processed.train_text,\n",
        "\n",
        "               SVM_model_sport,\n",
        "               CNN_model_sport,\n",
        "               Sport_processed.train_text,\n",
        "\n",
        "               SVM_model_economy,\n",
        "               CNN_model_economy,\n",
        "               Economy_processed.train_text,\n",
        "\n",
        "               SVM_model_art,\n",
        "               CNN_model_art,\n",
        "               Art_processed.train_text,\n",
        "\n",
        "               Pars_Bert_Model\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quLFkRRBclRY",
        "outputId": "a880c6e5-7325-4a8b-a2be-dd0f16014917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter your Text (Or Enter Quit): سامسونگ از جدید ترین گوشی خود رونمایی کرد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): tech\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "input text:  سامسونگ از جدید‌ترین گوشی خود رونمایی کرد\n",
            "CNN model output:  [[0.02772751]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 23.096371173858643 Seconds\n",
            "Please Enter your Text (Or Enter Quit): سرور های سامانه سوخت ایران با مشکل مواجه شدند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): tech\n",
            "1/1 [==============================] - 0s 376ms/step\n",
            "input text:  سرور‌های سامانه سوخت ایران با مشکل مواجه شدند\n",
            "CNN model output:  [[0.02724305]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 2.6027352809906006 Seconds\n",
            "Please Enter your Text (Or Enter Quit): علی دایی جایزه بهترین بازیکن سال اروپا را دریافت کرد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "input text:  علی دایی جایزه بهترین بازیکن سال اروپا را دریافت کرد\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 0.5552005767822266 Seconds\n",
            "Please Enter your Text (Or Enter Quit): تیم ملی ایران در دیداری دوستانه به مصاف امریکا خواهد رفت\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "input text:  تیم ملی ایران در دیداری دوستانه به مصاف امریکا خواهد رفت\n",
            "CNN model output:  [[0.8699171]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.587794303894043 Seconds\n",
            "Please Enter your Text (Or Enter Quit): آزمایش دوپینگ بازیکن استقلال مثبت شد و این بازیکن به کمیته انصباطی دعوت شد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb794737830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 230ms/step\n",
            "input text:  آزمایش دوپینگ بازیکن استقلال مثبت شد و این بازیکن به کمیته انصباطی دعوت شد\n",
            "CNN model output:  [[0.6380742]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 0.8045134544372559 Seconds\n",
            "Please Enter your Text (Or Enter Quit): شاهد بازی بسیار خسته کننده ای بودیم\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "input text:  شاهد بازی بسیار خسته کننده‌ای بودیم\n",
            "CNN model output:  [[0.07941524]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.5063791275024414 Seconds\n",
            "Please Enter your Text (Or Enter Quit): آمریکا هیچ غلطی نمیتواند بکند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb794707f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 157ms/step\n",
            "input text:  آمریکا هیچ غلطی نمیتواند بکند\n",
            "CNN model output:  [[0.47550246]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.175750494003296 Seconds\n",
            "Please Enter your Text (Or Enter Quit): مجوز کنسرت شادمهر عقیلی صادر نشد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "input text:  مجوز کنسرت شادمهر عقیلی صادر نشد\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.9643993377685547 Seconds\n",
            "Please Enter your Text (Or Enter Quit): اثر زیبایی کشف شد که تا به حال دیده نشده بود\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "input text:  اثر زیبایی کشف شد که تا به حال دیده نشده بود\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.9587509632110596 Seconds\n",
            "Please Enter your Text (Or Enter Quit): موادی کشف شد که تا به حال دیده نشده بود\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "input text:  موادی کشف شد که تا به حال دیده نشده بود\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.9015178680419922 Seconds\n",
            "Please Enter your Text (Or Enter Quit): مرغ گران شد و مردم به خیابان ها ریختند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "input text:  مرغ گران شد و مردم به خیابان‌ها ریختند\n",
            "CNN model output:  [[0.3015664]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.0672929286956787 Seconds\n",
            "Please Enter your Text (Or Enter Quit): شاهد موج گرانی و افزایش تورم در تمامی محصولات خواهیم بود\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "input text:  شاهد موج گرانی و افزایش تورم در تمامی محصولات خواهیم بود\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.0247371196746826 Seconds\n",
            "Please Enter your Text (Or Enter Quit): در چند روز آینده خبر های زیادی درباره افزایش قیمت محصولات خواهیم شندی\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "input text:  در چند روز آینده خبر‌های زیادی درباره افزایش قیمت محصولات خواهیم شندی\n",
            "CNN model output:  [[0.9997714]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.0384242534637451 Seconds\n",
            "Please Enter your Text (Or Enter Quit): خبر های موثقی به دست ما رسیده که در هفته حاری قیمت ها افزایش پیدا میکند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "input text:  خبر‌های موثقی به دست ما رسیده که در هفته حاری قیمت‌ها افزایش پیدا میکند\n",
            "CNN model output:  [[0.9978765]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.0949525833129883 Seconds\n",
            "Please Enter your Text (Or Enter Quit): با کاهش قیمت محصولات ایرانخودرو مردم استقبال شدیدی از محصولات این کارهونه کردند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): ecnomy\n",
            "Please Enter your Text (Or Enter Quit): با کاهش قیمت محصولات ایرانخودرو مردم استقبال شدیدی از محصولات این کارخونه کردند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "input text:  با کاهش قیمت محصولات ایرانخودرو مردم استقبال شدیدی از محصولات این کارخونه کردند\n",
            "CNN model output:  [[0.9999995]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.997732400894165 Seconds\n",
            "Please Enter your Text (Or Enter Quit): همه مردم ایران موظفند که دیگر مرغ نخرند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economt\n",
            "Please Enter your Text (Or Enter Quit): همه مردم ایران موظفند تا زمانی که قیمت ها کاهش پیدا نکند دیگر مرغ نخرند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "input text:  همه مردم ایران موظفند تا زمانی که قیمت‌ها کاهش پیدا نکند دیگر مرغ نخرند\n",
            "CNN model output:  [[0.9999453]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.9469709396362305 Seconds\n",
            "Please Enter your Text (Or Enter Quit): اهنگ جدید رضا صادقی منتشر شد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "input text:  اهنگ جدید رضا صادقی منتشر شد\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.88364839553833 Seconds\n",
            "Please Enter your Text (Or Enter Quit): مردم از آلبوم جدید خواننده مطرح استقبال نکردند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "input text:  مردم از آلبوم جدید خواننده مطرح استقبال نکردند\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.8355047702789307 Seconds\n",
            "Please Enter your Text (Or Enter Quit): کتاب عادل فردوسی بود به چاپ چهلم رسید\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "input text:  کتاب عادل فردوسی بود به چاپ چهلم رسید\n",
            "CNN model output:  [[0.07209528]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.87644624710083 Seconds\n",
            "Please Enter your Text (Or Enter Quit): اثر معروف داوینچی در حراجی به فروش رسید\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "input text:  اثر معروف داوینچی در حراجی به فروش رسید\n",
            "CNN model output:  [[0.22331078]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.834132194519043 Seconds\n",
            "Please Enter your Text (Or Enter Quit): یک تابلوی معروف و قشنگ از زیر زمین یک خانه در نیویورک کشف شد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): art\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "input text:  یک تابلوی معروف و قشنگ از زیر زمین یک خانه در نیویورک کشف شد\n",
            "CNN model output:  [[0.9917853]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 1.834636926651001 Seconds\n",
            "Please Enter your Text (Or Enter Quit): همه مردم ایران گوش کنید. همه ساعت 12 در میدان انقلاب جمع میشیم\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "input text:  همه مردم ایران گوش کنید همه ساعت در میدان انقلاب جمع میشیم\n",
            "CNN model output:  [[0.07898575]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.9797148704528809 Seconds\n",
            "Please Enter your Text (Or Enter Quit): همه مردم باید به گران شدن مرغ و گوشت اعتراض کنند\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "input text:  همه مردم باید به گران شدن مرغ و گوشت اعتراض کنند\n",
            "CNN model output:  [[0.41878024]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.9848060607910156 Seconds\n",
            "Please Enter your Text (Or Enter Quit): با این وضع اقتصادی هیچ کس توانایی حرید خانه را نحواهئ داشت\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "input text:  با این وضع اقتصادی هیچ کس توانایی حرید خانه را نحواهی داشت\n",
            "CNN model output:  [[0.07405071]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.9498586654663086 Seconds\n",
            "Please Enter your Text (Or Enter Quit): سرپرست پرسپولیس به رختکن داوران رفت که واکنش و اعتراض شدید تیم مقابل را به همراه دشات\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "input text:  سرپرست پرسپولیس به رختکن داوران رفت که واکنش و اعتراض شدید تیم مقابل را به همراه دشات\n",
            "CNN model output:  [[0.9999839]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 0.5391852855682373 Seconds\n",
            "Please Enter your Text (Or Enter Quit): کریستین اریکسن وسط بازی دچاز ایست قلبی شد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): sport\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "input text:  کریستین اریکسن وسط بازی دچاز ایست قلبی شد\n",
            "CNN model output:  [[0.67522264]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 0.49309515953063965 Seconds\n",
            "Please Enter your Text (Or Enter Quit): سرور های بانک ملی مورد حمله سایبری قرار گرفاتد\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): tech\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "input text:  سرور‌های بانک ملی مورد حمله سایبری قرار گرفاتد\n",
            "CNN model output:  [[1.]]\n",
            "SVM model output:  [1]\n",
            "Time Duration: 1.7596266269683838 Seconds\n",
            "Please Enter your Text (Or Enter Quit): بنظرتون توی هفته آینده محصولات غذایی گرون میشن؟\n",
            "Enter Category of Text (sport/ tech/ economy/ art/): economy\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "input text:  بنظرتون توی هفته آینده محصولات غذایی گرون میشن\n",
            "CNN model output:  [[0.14412281]]\n",
            "SVM model output:  [0]\n",
            "Time Duration: 0.9579057693481445 Seconds\n",
            "Please Enter your Text (Or Enter Quit): Quit\n"
          ]
        }
      ],
      "source": [
        "Real_time_Prediction.Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXx4HbvvUNjw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UjmLd8KFafKP",
        "haJgbMelaqGH",
        "tBiC_-_2a7jN",
        "8NioYnygWlzf",
        "ihTbRnJtXcSm",
        "tzfOvmZWW49y",
        "_RQwJqwjW5Cj",
        "2z8pPJwZW5IC",
        "D0euC6vyW5Nq",
        "yVHC2-q8XLU8",
        "P0cet-mCXLb9",
        "i3HWWCU5XLfU",
        "TbFnqfZyXLia",
        "9hzQXpUWXLln",
        "iquYqQDuYQ90",
        "MFEdaTPFW5bc",
        "Oj0xvKy5bIY8",
        "zs1AcBh3bw2S",
        "p0oUl8UXb2Ji",
        "XVJGRFiWb8G6",
        "Udh9G3-EcBuK",
        "k0g9uncWcPjL",
        "SudE1lfBchFx",
        "jVQsPQAQct04",
        "Ft3ivE_tuzg6",
        "1MM7ZIEzhH9S"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}